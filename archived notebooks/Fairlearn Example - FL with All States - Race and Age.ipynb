{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cedc37c",
   "metadata": {},
   "source": [
    "# Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "639928ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Data processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Models\n",
    "# LightGBM is a gradient boosting framework that uses tree based learning algorithms\n",
    "import lightgbm as lgb\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Fairlearn algorithms and utils\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from fairlearn.reductions import GridSearch, EqualizedOdds\n",
    "\n",
    "# Metrics\n",
    "from fairlearn.metrics import (\n",
    "    MetricFrame,\n",
    "    selection_rate, demographic_parity_difference, demographic_parity_ratio,\n",
    "    true_positive_rate, false_positive_rate, false_negative_rate,\n",
    "    false_positive_rate_difference, false_negative_rate_difference,\n",
    "    equalized_odds_difference)\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "26eda5d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>action_taken</th>\n",
       "      <th>preapproval_requested</th>\n",
       "      <th>loan_type</th>\n",
       "      <th>loan_purpose</th>\n",
       "      <th>interest_only_payment</th>\n",
       "      <th>balloon_payment</th>\n",
       "      <th>debt_to_income_ratio</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>loan_to_value_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ethnicity  race  gender  action_taken  preapproval_requested  loan_type  \\\n",
       "0          0     1       1             1                      0          1   \n",
       "1          0     1       1             1                      0          1   \n",
       "2          0     1       0             1                      0          1   \n",
       "3          0     1       0             1                      0          1   \n",
       "4          0     1       0             1                      0          1   \n",
       "\n",
       "   loan_purpose  interest_only_payment  balloon_payment  debt_to_income_ratio  \\\n",
       "0             1                      2                2                     3   \n",
       "1             1                      2                2                     4   \n",
       "2             1                      2                2                     4   \n",
       "3             3                      2                2                     2   \n",
       "4             1                      2                2                     2   \n",
       "\n",
       "   age  income  loan_to_value_ratio  \n",
       "0    3       1                    1  \n",
       "1    2       2                    3  \n",
       "2    2       3                    1  \n",
       "3    2       3                    1  \n",
       "4    3       2                    1  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the clean data\n",
    "fl = 'data/Fairlearn_FL_PortSL.csv'\n",
    "fl = pd.read_csv(fl, sep = ',')\n",
    "fl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1624bd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10606 entries, 0 to 10605\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count  Dtype\n",
      "---  ------                 --------------  -----\n",
      " 0   ethnicity              10606 non-null  int64\n",
      " 1   race                   10606 non-null  int64\n",
      " 2   gender                 10606 non-null  int64\n",
      " 3   action_taken           10606 non-null  int64\n",
      " 4   preapproval_requested  10606 non-null  int64\n",
      " 5   loan_type              10606 non-null  int64\n",
      " 6   loan_purpose           10606 non-null  int64\n",
      " 7   interest_only_payment  10606 non-null  int64\n",
      " 8   balloon_payment        10606 non-null  int64\n",
      " 9   debt_to_income_ratio   10606 non-null  int64\n",
      " 10  age                    10606 non-null  int64\n",
      " 11  income                 10606 non-null  int64\n",
      " 12  loan_to_value_ratio    10606 non-null  int64\n",
      "dtypes: int64(13)\n",
      "memory usage: 1.1 MB\n"
     ]
    }
   ],
   "source": [
    "fl.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8c904690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>action_taken</th>\n",
       "      <th>preapproval_requested</th>\n",
       "      <th>loan_type</th>\n",
       "      <th>loan_purpose</th>\n",
       "      <th>interest_only_payment</th>\n",
       "      <th>balloon_payment</th>\n",
       "      <th>debt_to_income_ratio</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>loan_to_value_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ethnicity  race  gender  action_taken  preapproval_requested  loan_type  \\\n",
       "0          0     1       0             1                      0          1   \n",
       "1          0     1       1             1                      0          3   \n",
       "2          0     2       0             1                      0          1   \n",
       "3          0     1       0             1                      0          1   \n",
       "4          0     3       0             1                      0          1   \n",
       "\n",
       "   loan_purpose  interest_only_payment  balloon_payment  debt_to_income_ratio  \\\n",
       "0             1                      2                2                     3   \n",
       "1             3                      2                2                     4   \n",
       "2             3                      2                2                     4   \n",
       "3             3                      2                2                     2   \n",
       "4             1                      2                2                     4   \n",
       "\n",
       "   age  income  loan_to_value_ratio  \n",
       "0    3       1                    1  \n",
       "1    3       3                    1  \n",
       "2    2       3                    3  \n",
       "3    3       5                    1  \n",
       "4    3       2                    1  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the clean data\n",
    "dc = 'data/Fairlearn_DC.csv'\n",
    "dc = pd.read_csv(dc, sep = ',')\n",
    "dc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b3a9abb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>action_taken</th>\n",
       "      <th>preapproval_requested</th>\n",
       "      <th>loan_type</th>\n",
       "      <th>loan_purpose</th>\n",
       "      <th>interest_only_payment</th>\n",
       "      <th>balloon_payment</th>\n",
       "      <th>debt_to_income_ratio</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>loan_to_value_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ethnicity  race  gender  action_taken  preapproval_requested  loan_type  \\\n",
       "0          0     1       1             1                      0          1   \n",
       "1          0     1       1             0                      0          1   \n",
       "2          1     1       1             1                      0          1   \n",
       "3          0     1       1             1                      0          1   \n",
       "4          1     1       0             1                      0          1   \n",
       "\n",
       "   loan_purpose  interest_only_payment  balloon_payment  debt_to_income_ratio  \\\n",
       "0             1                      2                2                     4   \n",
       "1             3                      2                2                     4   \n",
       "2             1                      2                2                     4   \n",
       "3             1                      2                2                     1   \n",
       "4             1                      2                2                     4   \n",
       "\n",
       "   age  income  loan_to_value_ratio  \n",
       "0    1       2                    3  \n",
       "1    2       2                    1  \n",
       "2    2       3                    1  \n",
       "3    3       3                    3  \n",
       "4    2       4                    1  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the clean data\n",
    "tx = 'data/Fairlearn_TX_Waco.csv'\n",
    "tx = pd.read_csv(tx, sep = ',')\n",
    "tx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eb74e5ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>action_taken</th>\n",
       "      <th>preapproval_requested</th>\n",
       "      <th>loan_type</th>\n",
       "      <th>loan_purpose</th>\n",
       "      <th>interest_only_payment</th>\n",
       "      <th>balloon_payment</th>\n",
       "      <th>debt_to_income_ratio</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>loan_to_value_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ethnicity  race  gender  action_taken  preapproval_requested  loan_type  \\\n",
       "0          0     1       0             1                      0          1   \n",
       "1          0     1       1             0                      0          1   \n",
       "2          0     1       1             1                      0          1   \n",
       "3          0     1       0             1                      0          1   \n",
       "4          1     1       1             1                      0          1   \n",
       "\n",
       "   loan_purpose  interest_only_payment  balloon_payment  debt_to_income_ratio  \\\n",
       "0             3                      2                2                     3   \n",
       "1             2                      2                2                     4   \n",
       "2             1                      2                2                     3   \n",
       "3             3                      2                2                     2   \n",
       "4             3                      2                2                     4   \n",
       "\n",
       "   age  income  loan_to_value_ratio  \n",
       "0    2       2                    1  \n",
       "1    3       4                    1  \n",
       "2    2       1                    1  \n",
       "3    2       2                    1  \n",
       "4    2       3                    1  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the clean data\n",
    "il = 'data/Fairlearn_IL_Chicago.csv'\n",
    "il = pd.read_csv(il, sep = ',')\n",
    "il.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c8747863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set = DC + TX + IL, testing set = FL\n",
    "all_df = pd.concat([tx, dc, il, fl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "77de1880",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 212714 entries, 0 to 10605\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count   Dtype\n",
      "---  ------                 --------------   -----\n",
      " 0   ethnicity              212714 non-null  int64\n",
      " 1   race                   212714 non-null  int64\n",
      " 2   gender                 212714 non-null  int64\n",
      " 3   action_taken           212714 non-null  int64\n",
      " 4   preapproval_requested  212714 non-null  int64\n",
      " 5   loan_type              212714 non-null  int64\n",
      " 6   loan_purpose           212714 non-null  int64\n",
      " 7   interest_only_payment  212714 non-null  int64\n",
      " 8   balloon_payment        212714 non-null  int64\n",
      " 9   debt_to_income_ratio   212714 non-null  int64\n",
      " 10  age                    212714 non-null  int64\n",
      " 11  income                 212714 non-null  int64\n",
      " 12  loan_to_value_ratio    212714 non-null  int64\n",
      "dtypes: int64(13)\n",
      "memory usage: 22.7 MB\n"
     ]
    }
   ],
   "source": [
    "all_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2e19f9a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            White\n",
       "1            White\n",
       "2            White\n",
       "3            White\n",
       "4            White\n",
       "           ...    \n",
       "10601    Non-White\n",
       "10602        White\n",
       "10603        White\n",
       "10604        White\n",
       "10605        White\n",
       "Name: race, Length: 212714, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the sensitive feature - example: race\n",
    "A = all_df[\"race\"].apply(lambda x:1 if x == 1 else 0)\n",
    "A_str = A.map({ 1:\"White\", 0:\"Non-White\"})\n",
    "A_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d996aa14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 212714 entries, 0 to 10605\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count   Dtype   \n",
      "---  ------                 --------------   -----   \n",
      " 0   ethnicity              212714 non-null  int64   \n",
      " 1   race                   212714 non-null  int64   \n",
      " 2   gender                 212714 non-null  int64   \n",
      " 3   action_taken           212714 non-null  int64   \n",
      " 4   preapproval_requested  212714 non-null  category\n",
      " 5   loan_type              212714 non-null  category\n",
      " 6   loan_purpose           212714 non-null  category\n",
      " 7   interest_only_payment  212714 non-null  category\n",
      " 8   balloon_payment        212714 non-null  category\n",
      " 9   debt_to_income_ratio   212714 non-null  category\n",
      " 10  age                    212714 non-null  int64   \n",
      " 11  income                 212714 non-null  category\n",
      " 12  loan_to_value_ratio    212714 non-null  int64   \n",
      "dtypes: category(7), int64(6)\n",
      "memory usage: 12.8 MB\n"
     ]
    }
   ],
   "source": [
    "# Extract the target\n",
    "Y = all_df[\"action_taken\"]\n",
    "categorical_features = ['preapproval_requested', 'loan_type','loan_purpose', 'interest_only_payment', 'balloon_payment', 'debt_to_income_ratio', 'income']\n",
    "for col in categorical_features:\n",
    "    all_df[col] = all_df[col].astype('category')\n",
    "all_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "204e5c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192109"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "212714-20605"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5656aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "df_train = all_df.drop(columns=['race', 'action_taken', 'ethnicity', 'gender', 'age']).head(192109)\n",
    "df_test = all_df.drop(columns=['race', 'action_taken', 'ethnicity', 'gender', 'age']).tail(10605)\n",
    "Y_train = Y.head(192109)\n",
    "Y_test = Y.tail(10605)\n",
    "A_train = A.head(192109)\n",
    "A_test = A.tail(10605)\n",
    "A_str_train = A_str.head(192109)\n",
    "A_str_test = A_str.tail(10605)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8d09fb70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 192109 entries, 0 to 100882\n",
      "Data columns (total 8 columns):\n",
      " #   Column                 Non-Null Count   Dtype   \n",
      "---  ------                 --------------   -----   \n",
      " 0   preapproval_requested  192109 non-null  category\n",
      " 1   loan_type              192109 non-null  category\n",
      " 2   loan_purpose           192109 non-null  category\n",
      " 3   interest_only_payment  192109 non-null  category\n",
      " 4   balloon_payment        192109 non-null  category\n",
      " 5   debt_to_income_ratio   192109 non-null  category\n",
      " 6   income                 192109 non-null  category\n",
      " 7   loan_to_value_ratio    192109 non-null  int64   \n",
      "dtypes: category(7), int64(1)\n",
      "memory usage: 4.2 MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1888afba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10605 entries, 1 to 10605\n",
      "Data columns (total 8 columns):\n",
      " #   Column                 Non-Null Count  Dtype   \n",
      "---  ------                 --------------  -----   \n",
      " 0   preapproval_requested  10605 non-null  category\n",
      " 1   loan_type              10605 non-null  category\n",
      " 2   loan_purpose           10605 non-null  category\n",
      " 3   interest_only_payment  10605 non-null  category\n",
      " 4   balloon_payment        10605 non-null  category\n",
      " 5   debt_to_income_ratio   10605 non-null  category\n",
      " 6   income                 10605 non-null  category\n",
      " 7   loan_to_value_ratio    10605 non-null  int64   \n",
      "dtypes: category(7), int64(1)\n",
      "memory usage: 239.3 KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec8c5cb",
   "metadata": {},
   "source": [
    "## Using a Fairness Unaware Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "69a19813",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'objective' : 'binary',\n",
    "    'metric' : 'auc',\n",
    "    'learning_rate': 0.03,\n",
    "    'num_leaves' : 10,\n",
    "    'max_depth' : 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1ded3996",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(**lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "74cffc68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(learning_rate=0.03, max_depth=3, metric='auc', num_leaves=10,\n",
       "               objective='binary')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(df_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cde8e4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores on test set\n",
    "test_scores = model.predict_proba(df_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "37a4761a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.839283421384989"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train AUC\n",
    "roc_auc_score(Y_train, model.predict_proba(df_train)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "acf9263c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions (0 or 1) on test set\n",
    "test_preds = (test_scores >= np.mean(Y_train)) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "08156ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LightGBM feature importance \n",
    "lgb.plot_importance(model, height=0.6, title=\"Features importance (LightGBM)\", importance_type=\"gain\", max_num_features=15) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ffffc3",
   "metadata": {},
   "source": [
    "We next use Fairlearn's `MetricFrame` to examine the the two different kinds of errors (false positives and false negatives) on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a520ac03",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\metrics\\_metric_frame.py:63: FutureWarning: You have provided 'metrics', 'y_true', 'y_pred' as positional arguments. Please pass them as keyword arguments. From version 0.10.0 passing them as positional arguments will result in an error.\n",
      "  warnings.warn(f\"You have provided {args_msg} as positional arguments. \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FPR</th>\n",
       "      <th>FNR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Non-White</th>\n",
       "      <td>0.353125</td>\n",
       "      <td>0.143333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White</th>\n",
       "      <td>0.306186</td>\n",
       "      <td>0.187327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                FPR       FNR\n",
       "race                         \n",
       "Non-White  0.353125  0.143333\n",
       "White      0.306186  0.187327"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mf = MetricFrame({\n",
    "    'FPR': false_positive_rate,\n",
    "    'FNR': false_negative_rate},\n",
    "    Y_test, test_preds, sensitive_features=A_str_test)\n",
    "\n",
    "mf.by_group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8f9df958",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9ba60ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_series = pd.Series(test_preds)\n",
    "test_preds_series.reset_index(drop=True, inplace=True)\n",
    "A_str_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7fba1363",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_white = pd.concat([test_preds_series , A_str_test], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "85c53bc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10600</th>\n",
       "      <td>0</td>\n",
       "      <td>Non-White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10601</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10602</th>\n",
       "      <td>0</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10603</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10604</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10605 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       race\n",
       "0      1      White\n",
       "1      1      White\n",
       "2      1      White\n",
       "3      1      White\n",
       "4      1      White\n",
       "...   ..        ...\n",
       "10600  0  Non-White\n",
       "10601  1      White\n",
       "10602  0      White\n",
       "10603  1      White\n",
       "10604  1      White\n",
       "\n",
       "[10605 rows x 2 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "69ea7bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_white.columns =['Unmitigated Result', 'Race']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ce01865f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unmitigated Result  Race     \n",
       "0                   Non-White     586\n",
       "                    White        2780\n",
       "1                   Non-White    1254\n",
       "                    White        5985\n",
       "Name: Race, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_white.groupby(['Unmitigated Result','Race'])['Race'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cf8fd828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def get_metrics_df(models_dict, y_true, group):\n",
    "    metrics_dict = {\n",
    "        \"Overall selection rate\": (\n",
    "            lambda x: selection_rate(y_true, x), True),\n",
    "        \"Demographic parity difference\": (\n",
    "            lambda x: demographic_parity_difference(y_true, x, sensitive_features=group), True),\n",
    "        \"Demographic parity ratio\": (\n",
    "            lambda x: demographic_parity_ratio(y_true, x, sensitive_features=group), True),\n",
    "        \"------\": (lambda x: \"\", True),\n",
    "        \"Overall balanced error rate\": (\n",
    "            lambda x: balanced_accuracy_score(y_true, x), True),\n",
    "        \"Balanced error rate difference\": (\n",
    "            lambda x: MetricFrame(metrics=balanced_accuracy_score, y_true=y_true, y_pred=x, sensitive_features=group).difference(method='between_groups'), True),\n",
    "        \" ------\": (lambda x: \"\", True),\n",
    "        # defined by team6 - equal opportunity - the percentage of people that have rightfully benefitted from the loan model\n",
    "        \"True positive rate\": (\n",
    "            lambda x: true_positive_rate(y_true, x), True),\n",
    "        # defined by team6 - equalized odds - the percentage of people that have wrongfully benefitted from the loan model\n",
    "        \"False positive rate difference\": (\n",
    "            lambda x: false_positive_rate_difference(y_true, x, sensitive_features=group), True),\n",
    "        \"False negative rate difference\": (\n",
    "            lambda x: false_negative_rate_difference(y_true, x, sensitive_features=group), True),\n",
    "        # defined by UCI Ex - 0 means that all groups have the same true positive, true negative, false positive, and false negative rates.)\n",
    "        \"Equalized odds difference\": (\n",
    "            lambda x: equalized_odds_difference(y_true, x, sensitive_features=group), True),\n",
    "        \"  ------\": (lambda x: \"\", True),\n",
    "        \"Overall AUC\": (\n",
    "            lambda x: roc_auc_score(y_true, x), False),\n",
    "        \"AUC difference\": (\n",
    "            lambda x: MetricFrame(metrics=roc_auc_score, y_true=y_true, y_pred=x, sensitive_features=group).difference(method='between_groups'), False)\n",
    "        \n",
    "    }\n",
    "    df_dict = {}\n",
    "    for metric_name, (metric_func, use_preds) in metrics_dict.items():\n",
    "        df_dict[metric_name] = [metric_func(preds) if use_preds else metric_func(scores) \n",
    "                                for model_name, (preds, scores) in models_dict.items()]\n",
    "    return pd.DataFrame.from_dict(df_dict, orient=\"index\", columns=models_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f0df44",
   "metadata": {},
   "source": [
    "We calculate several performance and fairness metrics below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d7642a4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unmitigated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Overall selection rate</th>\n",
       "      <td>0.682603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demographic parity difference</th>\n",
       "      <td>0.001308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demographic parity ratio</th>\n",
       "      <td>0.998085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall balanced error rate</th>\n",
       "      <td>0.751461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balanced error rate difference</th>\n",
       "      <td>0.001472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True positive rate</th>\n",
       "      <td>0.819513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False positive rate difference</th>\n",
       "      <td>0.046939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False negative rate difference</th>\n",
       "      <td>0.043994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equalized odds difference</th>\n",
       "      <td>0.046939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall AUC</th>\n",
       "      <td>0.82961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC difference</th>\n",
       "      <td>0.01459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Unmitigated\n",
       "Overall selection rate            0.682603\n",
       "Demographic parity difference     0.001308\n",
       "Demographic parity ratio          0.998085\n",
       "------                                    \n",
       "Overall balanced error rate       0.751461\n",
       "Balanced error rate difference    0.001472\n",
       " ------                                   \n",
       "True positive rate                0.819513\n",
       "False positive rate difference    0.046939\n",
       "False negative rate difference    0.043994\n",
       "Equalized odds difference         0.046939\n",
       "  ------                                  \n",
       "Overall AUC                        0.82961\n",
       "AUC difference                     0.01459"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metrics\n",
    "models_dict = {\"Unmitigated\": (test_preds, test_scores)}\n",
    "get_metrics_df(models_dict, Y_test, A_str_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd60941",
   "metadata": {},
   "source": [
    "## Mitigating Equalized Odds Difference with Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b266ba5b",
   "metadata": {},
   "source": [
    "We attempt to mitigate the disparities in the `lightgbm` predictions using the Fairlearn postprocessing algorithm `ThresholdOptimizer`. This algorithm finds a suitable threshold for the scores (class probabilities) produced by the `lightgbm` model by optimizing the accuracy rate under the constraint that the equalized odds difference (on training data) is zero. Since our goal is to optimize balanced accuracy, we resample the training data to have the same number of positive and negative examples. This means that `ThresholdOptimizer` is effectively optimizing balanced accuracy on the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bde592af",
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess_est = ThresholdOptimizer(\n",
    "    estimator=model,\n",
    "    constraints=\"equalized_odds\",\n",
    "    prefit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d5f05f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced data set is obtained by sampling the same number of points from the majority class (Y=0)\n",
    "# as there are points in the minority class (Y=1)\n",
    "balanced_idx1 = df_train[Y_train==1].index\n",
    "pp_train_idx = balanced_idx1.union(Y_train[Y_train==0].sample(n=balanced_idx1.size, random_state=1234, replace=True).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dbd3240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_balanced = df_train.loc[pp_train_idx, :]\n",
    "Y_train_balanced = Y_train.loc[pp_train_idx]\n",
    "A_train_balanced = A_train.loc[pp_train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7d52ebdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\postprocessing\\_threshold_optimizer.py:270: FutureWarning: 'predict_method' default value is changed from 'predict' to 'auto'. Explicitly pass `predict_method='predict' to replicate the old behavior, or pass `predict_method='auto' or other valid values to silence this warning.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ThresholdOptimizer(constraints='equalized_odds',\n",
       "                   estimator=LGBMClassifier(learning_rate=0.03, max_depth=3,\n",
       "                                            metric='auc', num_leaves=10,\n",
       "                                            objective='binary'),\n",
       "                   prefit=True)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postprocess_est.fit(df_train_balanced, Y_train_balanced, sensitive_features=A_train_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cc31b2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess_preds = postprocess_est.predict(df_test, sensitive_features=A_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2b5488df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unmitigated</th>\n",
       "      <th>ThresholdOptimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Overall selection rate</th>\n",
       "      <td>0.682603</td>\n",
       "      <td>0.867138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demographic parity difference</th>\n",
       "      <td>0.001308</td>\n",
       "      <td>0.03849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demographic parity ratio</th>\n",
       "      <td>0.998085</td>\n",
       "      <td>0.955952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall balanced error rate</th>\n",
       "      <td>0.751461</td>\n",
       "      <td>0.699762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balanced error rate difference</th>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.010338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True positive rate</th>\n",
       "      <td>0.819513</td>\n",
       "      <td>0.9759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False positive rate difference</th>\n",
       "      <td>0.046939</td>\n",
       "      <td>0.015822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False negative rate difference</th>\n",
       "      <td>0.043994</td>\n",
       "      <td>0.004854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equalized odds difference</th>\n",
       "      <td>0.046939</td>\n",
       "      <td>0.015822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall AUC</th>\n",
       "      <td>0.82961</td>\n",
       "      <td>0.699762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC difference</th>\n",
       "      <td>0.01459</td>\n",
       "      <td>0.010338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Unmitigated ThresholdOptimizer\n",
       "Overall selection rate            0.682603           0.867138\n",
       "Demographic parity difference     0.001308            0.03849\n",
       "Demographic parity ratio          0.998085           0.955952\n",
       "------                                                       \n",
       "Overall balanced error rate       0.751461           0.699762\n",
       "Balanced error rate difference    0.001472           0.010338\n",
       " ------                                                      \n",
       "True positive rate                0.819513             0.9759\n",
       "False positive rate difference    0.046939           0.015822\n",
       "False negative rate difference    0.043994           0.004854\n",
       "Equalized odds difference         0.046939           0.015822\n",
       "  ------                                                     \n",
       "Overall AUC                        0.82961           0.699762\n",
       "AUC difference                     0.01459           0.010338"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_dict = {\"Unmitigated\": (test_preds, test_scores),\n",
    "              \"ThresholdOptimizer\": (postprocess_preds, postprocess_preds)}\n",
    "get_metrics_df(models_dict, Y_test, A_str_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919f1698",
   "metadata": {},
   "source": [
    "The `ThresholdOptimizer` algorithm significantly reduces the disparity according to multiple metrics. However, the performance metrics (balanced error rate as well as AUC) get worse. Before deploying such a model in practice, it would be important to examine in more detail why we observe such a sharp trade-off. In our case it is because the available features are much less informative for one of the demographic groups than for the other.\n",
    "\n",
    "Note that unlike the unmitigated model, `ThresholdOptimizer` produces 0/1 predictions, so its balanced error rate difference is equal to the AUC difference, and its overall balanced error rate is equal to 1 - overall AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6712f85a",
   "metadata": {},
   "source": [
    "## Mitigating Equalized Odds Difference with GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22996117",
   "metadata": {},
   "source": [
    "We now attempt to mitigate disparities using the `GridSearch` algorithm. Unlike `ThresholdOptimizer`, the predictors produced by `GridSearch` do not access the sensitive feature at test time. Also, rather than training a single model, we train multiple models corresponding to different trade-off points between the performance metric (balanced accuracy) and fairness metric (equalized odds difference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "86022352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n"
     ]
    }
   ],
   "source": [
    "# Train GridSearch\n",
    "sweep = GridSearch(model,\n",
    "                   constraints=EqualizedOdds(),\n",
    "                   grid_size=1000,\n",
    "                   grid_limit=10)\n",
    "\n",
    "sweep.fit(df_train_balanced, Y_train_balanced, sensitive_features=A_train_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fdd2b76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_preds = [predictor.predict(df_test) for predictor in sweep.predictors_] \n",
    "sweep_scores = [predictor.predict_proba(df_test)[:, 1] for predictor in sweep.predictors_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e35a3b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized_odds_sweep = [\n",
    "    equalized_odds_difference(Y_test, preds, sensitive_features=A_str_test)\n",
    "    for preds in sweep_preds\n",
    "]\n",
    "balanced_accuracy_sweep = [balanced_accuracy_score(Y_test, preds) for preds in sweep_preds]\n",
    "auc_sweep = [roc_auc_score(Y_test, scores) for scores in sweep_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4914186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only non-dominated models (with respect to balanced accuracy and equalized odds difference)\n",
    "all_results = pd.DataFrame(\n",
    "    {\"predictor\": sweep.predictors_, \"accuracy\": balanced_accuracy_sweep, \"disparity\": equalized_odds_sweep}\n",
    ") \n",
    "non_dominated = [] \n",
    "for row in all_results.itertuples(): \n",
    "    accuracy_for_lower_or_eq_disparity = all_results[\"accuracy\"][all_results[\"disparity\"] <= row.disparity] \n",
    "    if row.accuracy >= accuracy_for_lower_or_eq_disparity.max(): \n",
    "        non_dominated.append(True)\n",
    "    else:\n",
    "        non_dominated.append(False)\n",
    "\n",
    "equalized_odds_sweep_non_dominated = np.asarray(equalized_odds_sweep)[non_dominated]\n",
    "balanced_accuracy_non_dominated = np.asarray(balanced_accuracy_sweep)[non_dominated]\n",
    "auc_non_dominated = np.asarray(auc_sweep)[non_dominated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "07127e31",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot equalized odds difference vs balanced accuracy\n",
    "plt.scatter(balanced_accuracy_non_dominated, equalized_odds_sweep_non_dominated, label=\"GridSearch Models\")\n",
    "plt.scatter(balanced_accuracy_score(Y_test, test_preds),\n",
    "            equalized_odds_difference(Y_test, test_preds, sensitive_features=A_str_test), \n",
    "            label=\"Unmitigated Model\")\n",
    "plt.scatter(balanced_accuracy_score(Y_test, postprocess_preds), \n",
    "            equalized_odds_difference(Y_test, postprocess_preds, sensitive_features=A_str_test),\n",
    "            label=\"ThresholdOptimizer Model\")\n",
    "plt.xlabel(\"Balanced Accuracy\")\n",
    "plt.ylabel(\"Equalized Odds Difference\")\n",
    "plt.legend(bbox_to_anchor=(1.55, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dff63d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21b386cb",
   "metadata": {},
   "source": [
    "As intended, `GridSearch` models appear along the trade-off curve between the large balanced accuracy (but also large disparity), and low disparity (but worse balanced accuracy). This gives the data scientist a flexibility to select a model that fits the application context best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6f895794",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot equalized odds difference vs AUC\n",
    "plt.scatter(auc_non_dominated, equalized_odds_sweep_non_dominated, label=\"GridSearch Models\")\n",
    "plt.scatter(roc_auc_score(Y_test, test_scores),\n",
    "            equalized_odds_difference(Y_test, test_preds, sensitive_features=A_str_test), \n",
    "            label=\"Unmitigated Model\")\n",
    "plt.scatter(roc_auc_score(Y_test, postprocess_preds), \n",
    "            equalized_odds_difference(Y_test, postprocess_preds, sensitive_features=A_str_test),\n",
    "            label=\"ThresholdOptimizer Model\")\n",
    "plt.xlabel(\"AUC\")\n",
    "plt.ylabel(\"Equalized Odds Difference\")\n",
    "plt.legend(bbox_to_anchor=(1.55, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8638e4",
   "metadata": {},
   "source": [
    "Similarly, `GridSearch` models appear along the trade-off curve between AUC and equalized odds difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cc88de81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unmitigated</th>\n",
       "      <th>ThresholdOptimizer</th>\n",
       "      <th>GridSearch_344</th>\n",
       "      <th>GridSearch_420</th>\n",
       "      <th>GridSearch_421</th>\n",
       "      <th>GridSearch_461</th>\n",
       "      <th>GridSearch_505</th>\n",
       "      <th>GridSearch_548</th>\n",
       "      <th>GridSearch_590</th>\n",
       "      <th>GridSearch_591</th>\n",
       "      <th>GridSearch_630</th>\n",
       "      <th>GridSearch_632</th>\n",
       "      <th>GridSearch_669</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Overall selection rate</th>\n",
       "      <td>0.682603</td>\n",
       "      <td>0.867138</td>\n",
       "      <td>0.804149</td>\n",
       "      <td>0.815559</td>\n",
       "      <td>0.816407</td>\n",
       "      <td>0.812258</td>\n",
       "      <td>0.815559</td>\n",
       "      <td>0.817916</td>\n",
       "      <td>0.81537</td>\n",
       "      <td>0.789722</td>\n",
       "      <td>0.770297</td>\n",
       "      <td>0.760868</td>\n",
       "      <td>0.767562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demographic parity difference</th>\n",
       "      <td>0.001308</td>\n",
       "      <td>0.03849</td>\n",
       "      <td>0.039871</td>\n",
       "      <td>0.039209</td>\n",
       "      <td>0.029715</td>\n",
       "      <td>0.041134</td>\n",
       "      <td>0.029346</td>\n",
       "      <td>0.043377</td>\n",
       "      <td>0.041612</td>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.028503</td>\n",
       "      <td>0.010519</td>\n",
       "      <td>0.012043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demographic parity ratio</th>\n",
       "      <td>0.998085</td>\n",
       "      <td>0.955952</td>\n",
       "      <td>0.950841</td>\n",
       "      <td>0.952321</td>\n",
       "      <td>0.963831</td>\n",
       "      <td>0.949799</td>\n",
       "      <td>0.964241</td>\n",
       "      <td>0.94745</td>\n",
       "      <td>0.949414</td>\n",
       "      <td>0.973405</td>\n",
       "      <td>0.963233</td>\n",
       "      <td>0.986209</td>\n",
       "      <td>0.984352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall balanced error rate</th>\n",
       "      <td>0.751461</td>\n",
       "      <td>0.699762</td>\n",
       "      <td>0.728116</td>\n",
       "      <td>0.730244</td>\n",
       "      <td>0.732017</td>\n",
       "      <td>0.73107</td>\n",
       "      <td>0.734051</td>\n",
       "      <td>0.728532</td>\n",
       "      <td>0.728687</td>\n",
       "      <td>0.740098</td>\n",
       "      <td>0.739603</td>\n",
       "      <td>0.743358</td>\n",
       "      <td>0.741294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balanced error rate difference</th>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.010338</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.003095</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.005746</td>\n",
       "      <td>0.001979</td>\n",
       "      <td>0.003307</td>\n",
       "      <td>0.004617</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.003816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True positive rate</th>\n",
       "      <td>0.819513</td>\n",
       "      <td>0.9759</td>\n",
       "      <td>0.928349</td>\n",
       "      <td>0.940917</td>\n",
       "      <td>0.942731</td>\n",
       "      <td>0.938067</td>\n",
       "      <td>0.94299</td>\n",
       "      <td>0.942343</td>\n",
       "      <td>0.939881</td>\n",
       "      <td>0.920446</td>\n",
       "      <td>0.900751</td>\n",
       "      <td>0.893366</td>\n",
       "      <td>0.898938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False positive rate difference</th>\n",
       "      <td>0.046939</td>\n",
       "      <td>0.015822</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>0.007077</td>\n",
       "      <td>0.01131</td>\n",
       "      <td>0.008847</td>\n",
       "      <td>0.016205</td>\n",
       "      <td>0.002854</td>\n",
       "      <td>0.006414</td>\n",
       "      <td>0.026581</td>\n",
       "      <td>0.016484</td>\n",
       "      <td>0.029598</td>\n",
       "      <td>0.027175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False negative rate difference</th>\n",
       "      <td>0.043994</td>\n",
       "      <td>0.004854</td>\n",
       "      <td>0.001955</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.013541</td>\n",
       "      <td>0.002645</td>\n",
       "      <td>0.012247</td>\n",
       "      <td>0.003761</td>\n",
       "      <td>0.002819</td>\n",
       "      <td>0.021181</td>\n",
       "      <td>0.014898</td>\n",
       "      <td>0.036471</td>\n",
       "      <td>0.034808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equalized odds difference</th>\n",
       "      <td>0.046939</td>\n",
       "      <td>0.015822</td>\n",
       "      <td>0.001955</td>\n",
       "      <td>0.007077</td>\n",
       "      <td>0.013541</td>\n",
       "      <td>0.008847</td>\n",
       "      <td>0.016205</td>\n",
       "      <td>0.003761</td>\n",
       "      <td>0.006414</td>\n",
       "      <td>0.026581</td>\n",
       "      <td>0.016484</td>\n",
       "      <td>0.036471</td>\n",
       "      <td>0.034808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall AUC</th>\n",
       "      <td>0.82961</td>\n",
       "      <td>0.699762</td>\n",
       "      <td>0.808113</td>\n",
       "      <td>0.819247</td>\n",
       "      <td>0.823051</td>\n",
       "      <td>0.825391</td>\n",
       "      <td>0.828427</td>\n",
       "      <td>0.82573</td>\n",
       "      <td>0.82613</td>\n",
       "      <td>0.831137</td>\n",
       "      <td>0.827801</td>\n",
       "      <td>0.829008</td>\n",
       "      <td>0.829128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC difference</th>\n",
       "      <td>0.01459</td>\n",
       "      <td>0.010338</td>\n",
       "      <td>0.031248</td>\n",
       "      <td>0.025981</td>\n",
       "      <td>0.020765</td>\n",
       "      <td>0.023044</td>\n",
       "      <td>0.020228</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>0.018826</td>\n",
       "      <td>0.010887</td>\n",
       "      <td>0.013964</td>\n",
       "      <td>0.006063</td>\n",
       "      <td>0.002031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Unmitigated ThresholdOptimizer GridSearch_344  \\\n",
       "Overall selection rate            0.682603           0.867138       0.804149   \n",
       "Demographic parity difference     0.001308            0.03849       0.039871   \n",
       "Demographic parity ratio          0.998085           0.955952       0.950841   \n",
       "------                                                                         \n",
       "Overall balanced error rate       0.751461           0.699762       0.728116   \n",
       "Balanced error rate difference    0.001472           0.010338       0.000129   \n",
       " ------                                                                        \n",
       "True positive rate                0.819513             0.9759       0.928349   \n",
       "False positive rate difference    0.046939           0.015822       0.001697   \n",
       "False negative rate difference    0.043994           0.004854       0.001955   \n",
       "Equalized odds difference         0.046939           0.015822       0.001955   \n",
       "  ------                                                                       \n",
       "Overall AUC                        0.82961           0.699762       0.808113   \n",
       "AUC difference                     0.01459           0.010338       0.031248   \n",
       "\n",
       "                               GridSearch_420 GridSearch_421 GridSearch_461  \\\n",
       "Overall selection rate               0.815559       0.816407       0.812258   \n",
       "Demographic parity difference        0.039209       0.029715       0.041134   \n",
       "Demographic parity ratio             0.952321       0.963831       0.949799   \n",
       "------                                                                        \n",
       "Overall balanced error rate          0.730244       0.732017        0.73107   \n",
       "Balanced error rate difference       0.003095       0.001116       0.005746   \n",
       " ------                                                                       \n",
       "True positive rate                   0.940917       0.942731       0.938067   \n",
       "False positive rate difference       0.007077        0.01131       0.008847   \n",
       "False negative rate difference       0.000887       0.013541       0.002645   \n",
       "Equalized odds difference            0.007077       0.013541       0.008847   \n",
       "  ------                                                                      \n",
       "Overall AUC                          0.819247       0.823051       0.825391   \n",
       "AUC difference                       0.025981       0.020765       0.023044   \n",
       "\n",
       "                               GridSearch_505 GridSearch_548 GridSearch_590  \\\n",
       "Overall selection rate               0.815559       0.817916        0.81537   \n",
       "Demographic parity difference        0.029346       0.043377       0.041612   \n",
       "Demographic parity ratio             0.964241        0.94745       0.949414   \n",
       "------                                                                        \n",
       "Overall balanced error rate          0.734051       0.728532       0.728687   \n",
       "Balanced error rate difference       0.001979       0.003307       0.004617   \n",
       " ------                                                                       \n",
       "True positive rate                    0.94299       0.942343       0.939881   \n",
       "False positive rate difference       0.016205       0.002854       0.006414   \n",
       "False negative rate difference       0.012247       0.003761       0.002819   \n",
       "Equalized odds difference            0.016205       0.003761       0.006414   \n",
       "  ------                                                                      \n",
       "Overall AUC                          0.828427        0.82573        0.82613   \n",
       "AUC difference                       0.020228       0.020305       0.018826   \n",
       "\n",
       "                               GridSearch_591 GridSearch_630 GridSearch_632  \\\n",
       "Overall selection rate               0.789722       0.770297       0.760868   \n",
       "Demographic parity difference          0.0211       0.028503       0.010519   \n",
       "Demographic parity ratio             0.973405       0.963233       0.986209   \n",
       "------                                                                        \n",
       "Overall balanced error rate          0.740098       0.739603       0.743358   \n",
       "Balanced error rate difference         0.0027       0.000793       0.003436   \n",
       " ------                                                                       \n",
       "True positive rate                   0.920446       0.900751       0.893366   \n",
       "False positive rate difference       0.026581       0.016484       0.029598   \n",
       "False negative rate difference       0.021181       0.014898       0.036471   \n",
       "Equalized odds difference            0.026581       0.016484       0.036471   \n",
       "  ------                                                                      \n",
       "Overall AUC                          0.831137       0.827801       0.829008   \n",
       "AUC difference                       0.010887       0.013964       0.006063   \n",
       "\n",
       "                               GridSearch_669  \n",
       "Overall selection rate               0.767562  \n",
       "Demographic parity difference        0.012043  \n",
       "Demographic parity ratio             0.984352  \n",
       "------                                         \n",
       "Overall balanced error rate          0.741294  \n",
       "Balanced error rate difference       0.003816  \n",
       " ------                                        \n",
       "True positive rate                   0.898938  \n",
       "False positive rate difference       0.027175  \n",
       "False negative rate difference       0.034808  \n",
       "Equalized odds difference            0.034808  \n",
       "  ------                                       \n",
       "Overall AUC                          0.829128  \n",
       "AUC difference                       0.002031  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare GridSearch models with low values of equalized odds difference with the previously constructed models\n",
    "grid_search_dict = {\"GridSearch_{}\".format(i): (sweep_preds[i], sweep_scores[i])\n",
    "                    for i in range(len(sweep_preds))\n",
    "                    if non_dominated[i] and equalized_odds_sweep[i]<0.1}\n",
    "models_dict.update(grid_search_dict)\n",
    "get_metrics_df(models_dict, Y_test, A_str_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d461b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chose the model with AUC = 83.11% from grid search\n",
    "test_pred_gird_search_591 = grid_search_dict.get(\"GridSearch_591\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fac1bb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_gird_search_591_series = pd.Series(test_pred_gird_search_591)\n",
    "test_pred_gird_search_591_series.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ceb80481",
   "metadata": {},
   "outputs": [],
   "source": [
    "migigated_pred_white = pd.concat([test_pred_gird_search_591_series , A_str_test], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b7334fd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10600</th>\n",
       "      <td>0</td>\n",
       "      <td>Non-White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10601</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10602</th>\n",
       "      <td>0</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10603</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10604</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10605 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       race\n",
       "0      1      White\n",
       "1      1      White\n",
       "2      1      White\n",
       "3      1      White\n",
       "4      1      White\n",
       "...   ..        ...\n",
       "10600  0  Non-White\n",
       "10601  1      White\n",
       "10602  0      White\n",
       "10603  1      White\n",
       "10604  1      White\n",
       "\n",
       "[10605 rows x 2 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "migigated_pred_white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "625cede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "migigated_pred_white.columns =['Mitigated Result', 'Race']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "98731cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mitigated Result  Race     \n",
       "0                 Non-White     419\n",
       "                  White        1811\n",
       "1                 Non-White    1421\n",
       "                  White        6954\n",
       "Name: Race, dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "migigated_pred_white.groupby(['Mitigated Result','Race'])['Race'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77965c56",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88127abe",
   "metadata": {},
   "source": [
    "In this notebook, we explored how a fairness-unaware gradient boosted trees model performed on the classification task in contrast to the postprocessed `ThresholdOptimizer` model and the `GridSearch` model. The `ThresholdOptimizer` greatly reduced the disparity in performance across multiple fairness metrics. However the overall error rate and AUC for the `ThresholdOptimizer` model were worse compared to the fairness-unaware model. \n",
    "\n",
    "With the `GridSearch` algorithm, we trained multiple models that balance the trade-off between the balanced accuracy and the equalized odds fairness metric. After engaging with relevant stakeholders, the data scientist can deploy the model that balances the performance-fairness trade-off that meets the needs of the business."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ef4cfe",
   "metadata": {},
   "source": [
    "# Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02b330e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
