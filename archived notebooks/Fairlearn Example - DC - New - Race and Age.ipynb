{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cedc37c",
   "metadata": {},
   "source": [
    "# Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "639928ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Data processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Models\n",
    "# LightGBM is a gradient boosting framework that uses tree based learning algorithms\n",
    "import lightgbm as lgb\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Fairlearn algorithms and utils\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from fairlearn.reductions import GridSearch, EqualizedOdds\n",
    "\n",
    "# Metrics\n",
    "from fairlearn.metrics import (\n",
    "    MetricFrame,\n",
    "    selection_rate, demographic_parity_difference, demographic_parity_ratio,\n",
    "    true_positive_rate, false_positive_rate, false_negative_rate,\n",
    "    false_positive_rate_difference, false_negative_rate_difference,\n",
    "    equalized_odds_difference)\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26eda5d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>action_taken</th>\n",
       "      <th>preapproval_requested</th>\n",
       "      <th>loan_type</th>\n",
       "      <th>loan_purpose</th>\n",
       "      <th>interest_only_payment</th>\n",
       "      <th>balloon_payment</th>\n",
       "      <th>debt_to_income_ratio</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>loan_to_value_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ethnicity  race  gender  action_taken  preapproval_requested  loan_type  \\\n",
       "0          0     1       0             1                      0          1   \n",
       "1          0     1       1             1                      0          3   \n",
       "2          0     2       0             1                      0          1   \n",
       "3          0     1       0             1                      0          1   \n",
       "4          0     3       0             1                      0          1   \n",
       "\n",
       "   loan_purpose  interest_only_payment  balloon_payment  debt_to_income_ratio  \\\n",
       "0             1                      2                2                     3   \n",
       "1             3                      2                2                     4   \n",
       "2             3                      2                2                     4   \n",
       "3             3                      2                2                     2   \n",
       "4             1                      2                2                     4   \n",
       "\n",
       "   age  income  loan_to_value_ratio  \n",
       "0    3       1                    1  \n",
       "1    3       3                    1  \n",
       "2    2       3                    3  \n",
       "3    3       5                    1  \n",
       "4    3       2                    1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the clean data\n",
    "d = 'data/Fairlearn_DC.csv'\n",
    "d = pd.read_csv(d, sep = ',')\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a9abb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e19f9a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            White\n",
       "1            White\n",
       "2        Non-White\n",
       "3            White\n",
       "4        Non-White\n",
       "           ...    \n",
       "89150    Non-White\n",
       "89151        White\n",
       "89152        White\n",
       "89153    Non-White\n",
       "89154        White\n",
       "Name: race, Length: 89155, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the sensitive feature - example: race\n",
    "A = d[\"race\"].apply(lambda x:1 if x == 1 else 0)\n",
    "A_str = A.map({ 1:\"White\", 0:\"Non-White\"})\n",
    "A_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d996aa14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 89155 entries, 0 to 89154\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count  Dtype   \n",
      "---  ------                 --------------  -----   \n",
      " 0   ethnicity              89155 non-null  int64   \n",
      " 1   race                   89155 non-null  int64   \n",
      " 2   gender                 89155 non-null  int64   \n",
      " 3   action_taken           89155 non-null  int64   \n",
      " 4   preapproval_requested  89155 non-null  category\n",
      " 5   loan_type              89155 non-null  category\n",
      " 6   loan_purpose           89155 non-null  category\n",
      " 7   interest_only_payment  89155 non-null  category\n",
      " 8   balloon_payment        89155 non-null  category\n",
      " 9   debt_to_income_ratio   89155 non-null  category\n",
      " 10  age                    89155 non-null  int64   \n",
      " 11  income                 89155 non-null  category\n",
      " 12  loan_to_value_ratio    89155 non-null  int64   \n",
      "dtypes: category(7), int64(6)\n",
      "memory usage: 4.7 MB\n"
     ]
    }
   ],
   "source": [
    "# Extract the target\n",
    "Y = d[\"action_taken\"]\n",
    "categorical_features = ['preapproval_requested', 'loan_type','loan_purpose', 'interest_only_payment', 'balloon_payment', 'debt_to_income_ratio', 'income']\n",
    "for col in categorical_features:\n",
    "    d[col] = d[col].astype('category')\n",
    "d.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74ac01de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "df_train, df_test, Y_train, Y_test, A_train, A_test, A_str_train, A_str_test = train_test_split(\n",
    "    d.drop(columns=['race', 'action_taken', 'ethnicity', 'gender', 'age']), \n",
    "    Y, \n",
    "    A, \n",
    "    A_str,\n",
    "    test_size = 0.3, \n",
    "    random_state=12345,\n",
    "    stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "584471fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 62408 entries, 75761 to 33478\n",
      "Data columns (total 8 columns):\n",
      " #   Column                 Non-Null Count  Dtype   \n",
      "---  ------                 --------------  -----   \n",
      " 0   preapproval_requested  62408 non-null  category\n",
      " 1   loan_type              62408 non-null  category\n",
      " 2   loan_purpose           62408 non-null  category\n",
      " 3   interest_only_payment  62408 non-null  category\n",
      " 4   balloon_payment        62408 non-null  category\n",
      " 5   debt_to_income_ratio   62408 non-null  category\n",
      " 6   income                 62408 non-null  category\n",
      " 7   loan_to_value_ratio    62408 non-null  int64   \n",
      "dtypes: category(7), int64(1)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d09fb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 62408 entries, 75761 to 33478\n",
      "Data columns (total 8 columns):\n",
      " #   Column                 Non-Null Count  Dtype   \n",
      "---  ------                 --------------  -----   \n",
      " 0   preapproval_requested  62408 non-null  category\n",
      " 1   loan_type              62408 non-null  category\n",
      " 2   loan_purpose           62408 non-null  category\n",
      " 3   interest_only_payment  62408 non-null  category\n",
      " 4   balloon_payment        62408 non-null  category\n",
      " 5   debt_to_income_ratio   62408 non-null  category\n",
      " 6   income                 62408 non-null  category\n",
      " 7   loan_to_value_ratio    62408 non-null  int64   \n",
      "dtypes: category(7), int64(1)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec8c5cb",
   "metadata": {},
   "source": [
    "## Using a Fairness Unaware Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69a19813",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'objective' : 'binary',\n",
    "    'metric' : 'auc',\n",
    "    'learning_rate': 0.03,\n",
    "    'num_leaves' : 10,\n",
    "    'max_depth' : 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ded3996",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(**lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74cffc68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(learning_rate=0.03, max_depth=3, metric='auc', num_leaves=10,\n",
       "               objective='binary')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(df_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cde8e4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores on test set\n",
    "test_scores = model.predict_proba(df_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37a4761a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8444153179566672"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train AUC\n",
    "roc_auc_score(Y_train, model.predict_proba(df_train)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acf9263c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions (0 or 1) on test set\n",
    "test_preds = (test_scores >= np.mean(Y_train)) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08156ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LightGBM feature importance \n",
    "lgb.plot_importance(model, height=0.6, title=\"Features importance (LightGBM)\", importance_type=\"gain\", max_num_features=15) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ffffc3",
   "metadata": {},
   "source": [
    "We next use Fairlearn's `MetricFrame` to examine the the two different kinds of errors (false positives and false negatives) on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a520ac03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\metrics\\_metric_frame.py:63: FutureWarning: You have provided 'metrics', 'y_true', 'y_pred' as positional arguments. Please pass them as keyword arguments. From version 0.10.0 passing them as positional arguments will result in an error.\n",
      "  warnings.warn(f\"You have provided {args_msg} as positional arguments. \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FPR</th>\n",
       "      <th>FNR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Non-White</th>\n",
       "      <td>0.33186</td>\n",
       "      <td>0.146432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White</th>\n",
       "      <td>0.317308</td>\n",
       "      <td>0.139082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                FPR       FNR\n",
       "race                         \n",
       "Non-White   0.33186  0.146432\n",
       "White      0.317308  0.139082"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mf = MetricFrame({\n",
    "    'FPR': false_positive_rate,\n",
    "    'FNR': false_negative_rate},\n",
    "    Y_test, test_preds, sensitive_features=A_str_test)\n",
    "\n",
    "mf.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf8fd828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def get_metrics_df(models_dict, y_true, group):\n",
    "    metrics_dict = {\n",
    "        \"Overall selection rate\": (\n",
    "            lambda x: selection_rate(y_true, x), True),\n",
    "        \"Demographic parity difference\": (\n",
    "            lambda x: demographic_parity_difference(y_true, x, sensitive_features=group), True),\n",
    "        \"Demographic parity ratio\": (\n",
    "            lambda x: demographic_parity_ratio(y_true, x, sensitive_features=group), True),\n",
    "        \"------\": (lambda x: \"\", True),\n",
    "        \"Overall balanced error rate\": (\n",
    "            lambda x: balanced_accuracy_score(y_true, x), True),\n",
    "        \"Balanced error rate difference\": (\n",
    "            lambda x: MetricFrame(metrics=balanced_accuracy_score, y_true=y_true, y_pred=x, sensitive_features=group).difference(method='between_groups'), True),\n",
    "        \" ------\": (lambda x: \"\", True),\n",
    "        # defined by team6 - equal opportunity - the percentage of people that have rightfully benefitted from the loan model\n",
    "        \"True positive rate\": (\n",
    "            lambda x: true_positive_rate(y_true, x), True),\n",
    "        # defined by team6 - equalized odds - the percentage of people that have wrongfully benefitted from the loan model\n",
    "        \"False positive rate difference\": (\n",
    "            lambda x: false_positive_rate_difference(y_true, x, sensitive_features=group), True),\n",
    "        \"False negative rate difference\": (\n",
    "            lambda x: false_negative_rate_difference(y_true, x, sensitive_features=group), True),\n",
    "        # defined by UCI Ex - 0 means that all groups have the same true positive, true negative, false positive, and false negative rates.)\n",
    "        \"Equalized odds difference\": (\n",
    "            lambda x: equalized_odds_difference(y_true, x, sensitive_features=group), True),\n",
    "        \"  ------\": (lambda x: \"\", True),\n",
    "        \"Overall AUC\": (\n",
    "            lambda x: roc_auc_score(y_true, x), False),\n",
    "        \"AUC difference\": (\n",
    "            lambda x: MetricFrame(metrics=roc_auc_score, y_true=y_true, y_pred=x, sensitive_features=group).difference(method='between_groups'), False),\n",
    "    }\n",
    "    df_dict = {}\n",
    "    for metric_name, (metric_func, use_preds) in metrics_dict.items():\n",
    "        df_dict[metric_name] = [metric_func(preds) if use_preds else metric_func(scores) \n",
    "                                for model_name, (preds, scores) in models_dict.items()]\n",
    "    return pd.DataFrame.from_dict(df_dict, orient=\"index\", columns=models_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f0df44",
   "metadata": {},
   "source": [
    "We calculate several performance and fairness metrics below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7642a4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unmitigated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Overall selection rate</th>\n",
       "      <td>0.732082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demographic parity difference</th>\n",
       "      <td>0.065589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demographic parity ratio</th>\n",
       "      <td>0.913861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall balanced error rate</th>\n",
       "      <td>0.766138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balanced error rate difference</th>\n",
       "      <td>0.010951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True positive rate</th>\n",
       "      <td>0.857912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False positive rate difference</th>\n",
       "      <td>0.014552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False negative rate difference</th>\n",
       "      <td>0.00735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equalized odds difference</th>\n",
       "      <td>0.014552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall AUC</th>\n",
       "      <td>0.840792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC difference</th>\n",
       "      <td>0.007402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Unmitigated\n",
       "Overall selection rate            0.732082\n",
       "Demographic parity difference     0.065589\n",
       "Demographic parity ratio          0.913861\n",
       "------                                    \n",
       "Overall balanced error rate       0.766138\n",
       "Balanced error rate difference    0.010951\n",
       " ------                                   \n",
       "True positive rate                0.857912\n",
       "False positive rate difference    0.014552\n",
       "False negative rate difference     0.00735\n",
       "Equalized odds difference         0.014552\n",
       "  ------                                  \n",
       "Overall AUC                       0.840792\n",
       "AUC difference                    0.007402"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metrics\n",
    "models_dict = {\"Unmitigated\": (test_preds, test_scores)}\n",
    "get_metrics_df(models_dict, Y_test, A_str_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd60941",
   "metadata": {},
   "source": [
    "## Mitigating Equalized Odds Difference with Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b266ba5b",
   "metadata": {},
   "source": [
    "We attempt to mitigate the disparities in the `lightgbm` predictions using the Fairlearn postprocessing algorithm `ThresholdOptimizer`. This algorithm finds a suitable threshold for the scores (class probabilities) produced by the `lightgbm` model by optimizing the accuracy rate under the constraint that the equalized odds difference (on training data) is zero. Since our goal is to optimize balanced accuracy, we resample the training data to have the same number of positive and negative examples. This means that `ThresholdOptimizer` is effectively optimizing balanced accuracy on the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bde592af",
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess_est = ThresholdOptimizer(\n",
    "    estimator=model,\n",
    "    constraints=\"equalized_odds\",\n",
    "    prefit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5f05f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced data set is obtained by sampling the same number of points from the majority class (Y=0)\n",
    "# as there are points in the minority class (Y=1)\n",
    "balanced_idx1 = df_train[Y_train==1].index\n",
    "pp_train_idx = balanced_idx1.union(Y_train[Y_train==0].sample(n=balanced_idx1.size, random_state=1234, replace=True).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbd3240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_balanced = df_train.loc[pp_train_idx, :]\n",
    "Y_train_balanced = Y_train.loc[pp_train_idx]\n",
    "A_train_balanced = A_train.loc[pp_train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d52ebdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\postprocessing\\_threshold_optimizer.py:270: FutureWarning: 'predict_method' default value is changed from 'predict' to 'auto'. Explicitly pass `predict_method='predict' to replicate the old behavior, or pass `predict_method='auto' or other valid values to silence this warning.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ThresholdOptimizer(constraints='equalized_odds',\n",
       "                   estimator=LGBMClassifier(learning_rate=0.03, max_depth=3,\n",
       "                                            metric='auc', num_leaves=10,\n",
       "                                            objective='binary'),\n",
       "                   prefit=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postprocess_est.fit(df_train_balanced, Y_train_balanced, sensitive_features=A_train_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc31b2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess_preds = postprocess_est.predict(df_test, sensitive_features=A_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b5488df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unmitigated</th>\n",
       "      <th>ThresholdOptimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Overall selection rate</th>\n",
       "      <td>0.732082</td>\n",
       "      <td>0.877706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demographic parity difference</th>\n",
       "      <td>0.065589</td>\n",
       "      <td>0.051869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demographic parity ratio</th>\n",
       "      <td>0.913861</td>\n",
       "      <td>0.942427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall balanced error rate</th>\n",
       "      <td>0.766138</td>\n",
       "      <td>0.706362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balanced error rate difference</th>\n",
       "      <td>0.010951</td>\n",
       "      <td>0.003278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True positive rate</th>\n",
       "      <td>0.857912</td>\n",
       "      <td>0.975274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False positive rate difference</th>\n",
       "      <td>0.014552</td>\n",
       "      <td>0.007667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False negative rate difference</th>\n",
       "      <td>0.00735</td>\n",
       "      <td>0.001112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equalized odds difference</th>\n",
       "      <td>0.014552</td>\n",
       "      <td>0.007667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall AUC</th>\n",
       "      <td>0.840792</td>\n",
       "      <td>0.706362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC difference</th>\n",
       "      <td>0.007402</td>\n",
       "      <td>0.003278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Unmitigated ThresholdOptimizer\n",
       "Overall selection rate            0.732082           0.877706\n",
       "Demographic parity difference     0.065589           0.051869\n",
       "Demographic parity ratio          0.913861           0.942427\n",
       "------                                                       \n",
       "Overall balanced error rate       0.766138           0.706362\n",
       "Balanced error rate difference    0.010951           0.003278\n",
       " ------                                                      \n",
       "True positive rate                0.857912           0.975274\n",
       "False positive rate difference    0.014552           0.007667\n",
       "False negative rate difference     0.00735           0.001112\n",
       "Equalized odds difference         0.014552           0.007667\n",
       "  ------                                                     \n",
       "Overall AUC                       0.840792           0.706362\n",
       "AUC difference                    0.007402           0.003278"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_dict = {\"Unmitigated\": (test_preds, test_scores),\n",
    "              \"ThresholdOptimizer\": (postprocess_preds, postprocess_preds)}\n",
    "get_metrics_df(models_dict, Y_test, A_str_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919f1698",
   "metadata": {},
   "source": [
    "The `ThresholdOptimizer` algorithm significantly reduces the disparity according to multiple metrics. However, the performance metrics (balanced error rate as well as AUC) get worse. Before deploying such a model in practice, it would be important to examine in more detail why we observe such a sharp trade-off. In our case it is because the available features are much less informative for one of the demographic groups than for the other.\n",
    "\n",
    "Note that unlike the unmitigated model, `ThresholdOptimizer` produces 0/1 predictions, so its balanced error rate difference is equal to the AUC difference, and its overall balanced error rate is equal to 1 - overall AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6712f85a",
   "metadata": {},
   "source": [
    "## Mitigating Equalized Odds Difference with GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22996117",
   "metadata": {},
   "source": [
    "We now attempt to mitigate disparities using the `GridSearch` algorithm. Unlike `ThresholdOptimizer`, the predictors produced by `GridSearch` do not access the sensitive feature at test time. Also, rather than training a single model, we train multiple models corresponding to different trade-off points between the performance metric (balanced accuracy) and fairness metric (equalized odds difference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11b95a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n"
     ]
    }
   ],
   "source": [
    "# Train GridSearch\n",
    "sweep = GridSearch(model,\n",
    "                   constraints=EqualizedOdds(),\n",
    "                   grid_size=1000,\n",
    "                   grid_limit=10)\n",
    "\n",
    "sweep.fit(df_train_balanced, Y_train_balanced, sensitive_features=A_train_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fdd2b76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_preds = [predictor.predict(df_test) for predictor in sweep.predictors_] \n",
    "sweep_scores = [predictor.predict_proba(df_test)[:, 1] for predictor in sweep.predictors_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e35a3b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized_odds_sweep = [\n",
    "    equalized_odds_difference(Y_test, preds, sensitive_features=A_str_test)\n",
    "    for preds in sweep_preds\n",
    "]\n",
    "balanced_accuracy_sweep = [balanced_accuracy_score(Y_test, preds) for preds in sweep_preds]\n",
    "auc_sweep = [roc_auc_score(Y_test, scores) for scores in sweep_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4914186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only non-dominated models (with respect to balanced accuracy and equalized odds difference)\n",
    "all_results = pd.DataFrame(\n",
    "    {\"predictor\": sweep.predictors_, \"accuracy\": balanced_accuracy_sweep, \"disparity\": equalized_odds_sweep}\n",
    ") \n",
    "non_dominated = [] \n",
    "for row in all_results.itertuples(): \n",
    "    accuracy_for_lower_or_eq_disparity = all_results[\"accuracy\"][all_results[\"disparity\"] <= row.disparity] \n",
    "    if row.accuracy >= accuracy_for_lower_or_eq_disparity.max(): \n",
    "        non_dominated.append(True)\n",
    "    else:\n",
    "        non_dominated.append(False)\n",
    "\n",
    "equalized_odds_sweep_non_dominated = np.asarray(equalized_odds_sweep)[non_dominated]\n",
    "balanced_accuracy_non_dominated = np.asarray(balanced_accuracy_sweep)[non_dominated]\n",
    "auc_non_dominated = np.asarray(auc_sweep)[non_dominated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07127e31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot equalized odds difference vs balanced accuracy\n",
    "plt.scatter(balanced_accuracy_non_dominated, equalized_odds_sweep_non_dominated, label=\"GridSearch Models\")\n",
    "plt.scatter(balanced_accuracy_score(Y_test, test_preds),\n",
    "            equalized_odds_difference(Y_test, test_preds, sensitive_features=A_str_test), \n",
    "            label=\"Unmitigated Model\")\n",
    "plt.scatter(balanced_accuracy_score(Y_test, postprocess_preds), \n",
    "            equalized_odds_difference(Y_test, postprocess_preds, sensitive_features=A_str_test),\n",
    "            label=\"ThresholdOptimizer Model\")\n",
    "plt.xlabel(\"Balanced Accuracy\")\n",
    "plt.ylabel(\"Equalized Odds Difference\")\n",
    "plt.legend(bbox_to_anchor=(1.55, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b386cb",
   "metadata": {},
   "source": [
    "As intended, `GridSearch` models appear along the trade-off curve between the large balanced accuracy (but also large disparity), and low disparity (but worse balanced accuracy). This gives the data scientist a flexibility to select a model that fits the application context best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f895794",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot equalized odds difference vs AUC\n",
    "plt.scatter(auc_non_dominated, equalized_odds_sweep_non_dominated, label=\"GridSearch Models\")\n",
    "plt.scatter(roc_auc_score(Y_test, test_scores),\n",
    "            equalized_odds_difference(Y_test, test_preds, sensitive_features=A_str_test), \n",
    "            label=\"Unmitigated Model\")\n",
    "plt.scatter(roc_auc_score(Y_test, postprocess_preds), \n",
    "            equalized_odds_difference(Y_test, postprocess_preds, sensitive_features=A_str_test),\n",
    "            label=\"ThresholdOptimizer Model\")\n",
    "plt.xlabel(\"AUC\")\n",
    "plt.ylabel(\"Equalized Odds Difference\")\n",
    "plt.legend(bbox_to_anchor=(1.55, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8638e4",
   "metadata": {},
   "source": [
    "Similarly, `GridSearch` models appear along the trade-off curve between AUC and equalized odds difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc88de81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unmitigated</th>\n",
       "      <th>ThresholdOptimizer</th>\n",
       "      <th>GridSearch_421</th>\n",
       "      <th>GridSearch_505</th>\n",
       "      <th>GridSearch_506</th>\n",
       "      <th>GridSearch_548</th>\n",
       "      <th>GridSearch_549</th>\n",
       "      <th>GridSearch_590</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Overall selection rate</th>\n",
       "      <td>0.732082</td>\n",
       "      <td>0.877706</td>\n",
       "      <td>0.728269</td>\n",
       "      <td>0.71522</td>\n",
       "      <td>0.733129</td>\n",
       "      <td>0.712865</td>\n",
       "      <td>0.697125</td>\n",
       "      <td>0.695667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demographic parity difference</th>\n",
       "      <td>0.065589</td>\n",
       "      <td>0.051869</td>\n",
       "      <td>0.071388</td>\n",
       "      <td>0.07907</td>\n",
       "      <td>0.066426</td>\n",
       "      <td>0.078133</td>\n",
       "      <td>0.060679</td>\n",
       "      <td>0.057283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demographic parity ratio</th>\n",
       "      <td>0.913861</td>\n",
       "      <td>0.942427</td>\n",
       "      <td>0.906096</td>\n",
       "      <td>0.894659</td>\n",
       "      <td>0.912925</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.916222</td>\n",
       "      <td>0.920584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall balanced error rate</th>\n",
       "      <td>0.766138</td>\n",
       "      <td>0.706362</td>\n",
       "      <td>0.764573</td>\n",
       "      <td>0.769906</td>\n",
       "      <td>0.765581</td>\n",
       "      <td>0.765878</td>\n",
       "      <td>0.76251</td>\n",
       "      <td>0.759898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balanced error rate difference</th>\n",
       "      <td>0.010951</td>\n",
       "      <td>0.003278</td>\n",
       "      <td>0.009633</td>\n",
       "      <td>0.014221</td>\n",
       "      <td>0.010807</td>\n",
       "      <td>0.012227</td>\n",
       "      <td>0.006314</td>\n",
       "      <td>0.001844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True positive rate</th>\n",
       "      <td>0.857912</td>\n",
       "      <td>0.975274</td>\n",
       "      <td>0.853359</td>\n",
       "      <td>0.842832</td>\n",
       "      <td>0.858696</td>\n",
       "      <td>0.838572</td>\n",
       "      <td>0.82124</td>\n",
       "      <td>0.818547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False positive rate difference</th>\n",
       "      <td>0.014552</td>\n",
       "      <td>0.007667</td>\n",
       "      <td>0.00625</td>\n",
       "      <td>0.006699</td>\n",
       "      <td>0.013345</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>0.011617</td>\n",
       "      <td>0.007638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False negative rate difference</th>\n",
       "      <td>0.00735</td>\n",
       "      <td>0.001112</td>\n",
       "      <td>0.013016</td>\n",
       "      <td>0.021742</td>\n",
       "      <td>0.00827</td>\n",
       "      <td>0.020815</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.003949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equalized odds difference</th>\n",
       "      <td>0.014552</td>\n",
       "      <td>0.007667</td>\n",
       "      <td>0.013016</td>\n",
       "      <td>0.021742</td>\n",
       "      <td>0.013345</td>\n",
       "      <td>0.020815</td>\n",
       "      <td>0.011617</td>\n",
       "      <td>0.007638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall AUC</th>\n",
       "      <td>0.840792</td>\n",
       "      <td>0.706362</td>\n",
       "      <td>0.839789</td>\n",
       "      <td>0.84291</td>\n",
       "      <td>0.841088</td>\n",
       "      <td>0.838532</td>\n",
       "      <td>0.841078</td>\n",
       "      <td>0.836187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC difference</th>\n",
       "      <td>0.007402</td>\n",
       "      <td>0.003278</td>\n",
       "      <td>0.007948</td>\n",
       "      <td>0.004834</td>\n",
       "      <td>0.007382</td>\n",
       "      <td>0.002106</td>\n",
       "      <td>0.004554</td>\n",
       "      <td>0.00168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Unmitigated ThresholdOptimizer GridSearch_421  \\\n",
       "Overall selection rate            0.732082           0.877706       0.728269   \n",
       "Demographic parity difference     0.065589           0.051869       0.071388   \n",
       "Demographic parity ratio          0.913861           0.942427       0.906096   \n",
       "------                                                                         \n",
       "Overall balanced error rate       0.766138           0.706362       0.764573   \n",
       "Balanced error rate difference    0.010951           0.003278       0.009633   \n",
       " ------                                                                        \n",
       "True positive rate                0.857912           0.975274       0.853359   \n",
       "False positive rate difference    0.014552           0.007667        0.00625   \n",
       "False negative rate difference     0.00735           0.001112       0.013016   \n",
       "Equalized odds difference         0.014552           0.007667       0.013016   \n",
       "  ------                                                                       \n",
       "Overall AUC                       0.840792           0.706362       0.839789   \n",
       "AUC difference                    0.007402           0.003278       0.007948   \n",
       "\n",
       "                               GridSearch_505 GridSearch_506 GridSearch_548  \\\n",
       "Overall selection rate                0.71522       0.733129       0.712865   \n",
       "Demographic parity difference         0.07907       0.066426       0.078133   \n",
       "Demographic parity ratio             0.894659       0.912925       0.895522   \n",
       "------                                                                        \n",
       "Overall balanced error rate          0.769906       0.765581       0.765878   \n",
       "Balanced error rate difference       0.014221       0.010807       0.012227   \n",
       " ------                                                                       \n",
       "True positive rate                   0.842832       0.858696       0.838572   \n",
       "False positive rate difference       0.006699       0.013345       0.003639   \n",
       "False negative rate difference       0.021742        0.00827       0.020815   \n",
       "Equalized odds difference            0.021742       0.013345       0.020815   \n",
       "  ------                                                                      \n",
       "Overall AUC                           0.84291       0.841088       0.838532   \n",
       "AUC difference                       0.004834       0.007382       0.002106   \n",
       "\n",
       "                               GridSearch_549 GridSearch_590  \n",
       "Overall selection rate               0.697125       0.695667  \n",
       "Demographic parity difference        0.060679       0.057283  \n",
       "Demographic parity ratio             0.916222       0.920584  \n",
       "------                                                        \n",
       "Overall balanced error rate           0.76251       0.759898  \n",
       "Balanced error rate difference       0.006314       0.001844  \n",
       " ------                                                       \n",
       "True positive rate                    0.82124       0.818547  \n",
       "False positive rate difference       0.011617       0.007638  \n",
       "False negative rate difference       0.001012       0.003949  \n",
       "Equalized odds difference            0.011617       0.007638  \n",
       "  ------                                                      \n",
       "Overall AUC                          0.841078       0.836187  \n",
       "AUC difference                       0.004554        0.00168  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare GridSearch models with low values of equalized odds difference with the previously constructed models\n",
    "grid_search_dict = {\"GridSearch_{}\".format(i): (sweep_preds[i], sweep_scores[i])\n",
    "                    for i in range(len(sweep_preds))\n",
    "                    if non_dominated[i] and equalized_odds_sweep[i]<0.1}\n",
    "models_dict.update(grid_search_dict)\n",
    "get_metrics_df(models_dict, Y_test, A_str_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77965c56",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88127abe",
   "metadata": {},
   "source": [
    "In this notebook, we explored how a fairness-unaware gradient boosted trees model performed on the classification task in contrast to the postprocessed `ThresholdOptimizer` model and the `GridSearch` model. The `ThresholdOptimizer` greatly reduced the disparity in performance across multiple fairness metrics. However the overall error rate and AUC for the `ThresholdOptimizer` model were worse compared to the fairness-unaware model. \n",
    "\n",
    "With the `GridSearch` algorithm, we trained multiple models that balance the trade-off between the balanced accuracy and the equalized odds fairness metric. After engaging with relevant stakeholders, the data scientist can deploy the model that balances the performance-fairness trade-off that meets the needs of the business."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ef4cfe",
   "metadata": {},
   "source": [
    "# Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82d5ce3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>action_taken</th>\n",
       "      <th>preapproval_requested</th>\n",
       "      <th>loan_type</th>\n",
       "      <th>loan_purpose</th>\n",
       "      <th>interest_only_payment</th>\n",
       "      <th>balloon_payment</th>\n",
       "      <th>debt_to_income_ratio</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>loan_to_value_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ethnicity  race  gender  action_taken  preapproval_requested  loan_type  \\\n",
       "0          0     1       0             1                      0          1   \n",
       "1          0     1       1             1                      0          3   \n",
       "2          0     2       0             1                      0          1   \n",
       "3          0     1       0             1                      0          1   \n",
       "4          0     3       0             1                      0          1   \n",
       "\n",
       "   loan_purpose  interest_only_payment  balloon_payment  debt_to_income_ratio  \\\n",
       "0             1                      2                2                     3   \n",
       "1             3                      2                2                     4   \n",
       "2             3                      2                2                     4   \n",
       "3             3                      2                2                     2   \n",
       "4             1                      2                2                     4   \n",
       "\n",
       "   age  income  loan_to_value_ratio  \n",
       "0    3       1                    1  \n",
       "1    3       3                    1  \n",
       "2    2       3                    3  \n",
       "3    3       5                    1  \n",
       "4    3       2                    1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the clean data\n",
    "d = 'data/Fairlearn_DC.csv'\n",
    "d = pd.read_csv(d, sep = ',')\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b02b330e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Non-Middle-Aged\n",
       "1        Non-Middle-Aged\n",
       "2            Middle-Aged\n",
       "3        Non-Middle-Aged\n",
       "4        Non-Middle-Aged\n",
       "              ...       \n",
       "89150    Non-Middle-Aged\n",
       "89151    Non-Middle-Aged\n",
       "89152        Middle-Aged\n",
       "89153    Non-Middle-Aged\n",
       "89154    Non-Middle-Aged\n",
       "Name: age, Length: 89155, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the sensitive feature - example: age - middle-aged vs non m-a\n",
    "B = d[\"age\"].apply(lambda x:1 if x == 2 else 0)\n",
    "B_str = B.map({ 1:\"Middle-Aged\", 0:\"Non-Middle-Aged\"})\n",
    "B_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12d62d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 89155 entries, 0 to 89154\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count  Dtype   \n",
      "---  ------                 --------------  -----   \n",
      " 0   ethnicity              89155 non-null  int64   \n",
      " 1   race                   89155 non-null  int64   \n",
      " 2   gender                 89155 non-null  int64   \n",
      " 3   action_taken           89155 non-null  int64   \n",
      " 4   preapproval_requested  89155 non-null  category\n",
      " 5   loan_type              89155 non-null  category\n",
      " 6   loan_purpose           89155 non-null  category\n",
      " 7   interest_only_payment  89155 non-null  category\n",
      " 8   balloon_payment        89155 non-null  category\n",
      " 9   debt_to_income_ratio   89155 non-null  category\n",
      " 10  age                    89155 non-null  int64   \n",
      " 11  income                 89155 non-null  category\n",
      " 12  loan_to_value_ratio    89155 non-null  int64   \n",
      "dtypes: category(7), int64(6)\n",
      "memory usage: 4.7 MB\n"
     ]
    }
   ],
   "source": [
    "# Extract the target\n",
    "Y = d[\"action_taken\"]\n",
    "categorical_features = ['preapproval_requested', 'loan_type','loan_purpose', 'interest_only_payment', 'balloon_payment', 'debt_to_income_ratio', 'income']\n",
    "for col in categorical_features:\n",
    "    d[col] = d[col].astype('category')\n",
    "d.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d17facd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "df_train, df_test, Y_train, Y_test, B_train, B_test, B_str_train, B_str_test = train_test_split(\n",
    "    d.drop(columns=['race', 'action_taken', 'ethnicity', 'gender', 'age']), \n",
    "    Y, \n",
    "    B, \n",
    "    B_str,\n",
    "    test_size = 0.3, \n",
    "    random_state=12345,\n",
    "    stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ed4ddfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 62408 entries, 75761 to 33478\n",
      "Data columns (total 8 columns):\n",
      " #   Column                 Non-Null Count  Dtype   \n",
      "---  ------                 --------------  -----   \n",
      " 0   preapproval_requested  62408 non-null  category\n",
      " 1   loan_type              62408 non-null  category\n",
      " 2   loan_purpose           62408 non-null  category\n",
      " 3   interest_only_payment  62408 non-null  category\n",
      " 4   balloon_payment        62408 non-null  category\n",
      " 5   debt_to_income_ratio   62408 non-null  category\n",
      " 6   income                 62408 non-null  category\n",
      " 7   loan_to_value_ratio    62408 non-null  int64   \n",
      "dtypes: category(7), int64(1)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ede136",
   "metadata": {},
   "source": [
    "## Using a Fairness Unaware Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0517e579",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'objective' : 'binary',\n",
    "    'metric' : 'auc',\n",
    "    'learning_rate': 0.03,\n",
    "    'num_leaves' : 10,\n",
    "    'max_depth' : 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd4280f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(**lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59644aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(learning_rate=0.03, max_depth=3, metric='auc', num_leaves=10,\n",
       "               objective='binary')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(df_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c95c6f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores on test set\n",
    "test_scores = model.predict_proba(df_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a767c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8444153179566672"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train AUC\n",
    "roc_auc_score(Y_train, model.predict_proba(df_train)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8542f255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions (0 or 1) on test set\n",
    "test_preds = (test_scores >= np.mean(Y_train)) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ceba1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LightGBM feature importance \n",
    "lgb.plot_importance(model, height=0.6, title=\"Features importance (LightGBM)\", importance_type=\"gain\", max_num_features=15) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21883bed",
   "metadata": {},
   "source": [
    "We next use Fairlearn's `MetricFrame` to examine the the two different kinds of errors (false positives and false negatives) on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "455c97fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\metrics\\_metric_frame.py:63: FutureWarning: You have provided 'metrics', 'y_true', 'y_pred' as positional arguments. Please pass them as keyword arguments. From version 0.10.0 passing them as positional arguments will result in an error.\n",
      "  warnings.warn(f\"You have provided {args_msg} as positional arguments. \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FPR</th>\n",
       "      <th>FNR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Middle-Aged</th>\n",
       "      <td>0.335514</td>\n",
       "      <td>0.143542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-Middle-Aged</th>\n",
       "      <td>0.315451</td>\n",
       "      <td>0.140507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      FPR       FNR\n",
       "age                                \n",
       "Middle-Aged      0.335514  0.143542\n",
       "Non-Middle-Aged  0.315451  0.140507"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mf = MetricFrame({\n",
    "    'FPR': false_positive_rate,\n",
    "    'FNR': false_negative_rate},\n",
    "    Y_test, test_preds, sensitive_features=B_str_test)\n",
    "\n",
    "mf.by_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f7c03c",
   "metadata": {},
   "source": [
    "We calculate several performance and fairness metrics below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dc5e42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unmitigated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Overall selection rate</th>\n",
       "      <td>0.732082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demographic parity difference</th>\n",
       "      <td>0.007506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demographic parity ratio</th>\n",
       "      <td>0.989798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall balanced error rate</th>\n",
       "      <td>0.766138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balanced error rate difference</th>\n",
       "      <td>0.011549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True positive rate</th>\n",
       "      <td>0.857912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False positive rate difference</th>\n",
       "      <td>0.020063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False negative rate difference</th>\n",
       "      <td>0.003035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equalized odds difference</th>\n",
       "      <td>0.020063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall AUC</th>\n",
       "      <td>0.840792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC difference</th>\n",
       "      <td>0.019052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Unmitigated\n",
       "Overall selection rate            0.732082\n",
       "Demographic parity difference     0.007506\n",
       "Demographic parity ratio          0.989798\n",
       "------                                    \n",
       "Overall balanced error rate       0.766138\n",
       "Balanced error rate difference    0.011549\n",
       " ------                                   \n",
       "True positive rate                0.857912\n",
       "False positive rate difference    0.020063\n",
       "False negative rate difference    0.003035\n",
       "Equalized odds difference         0.020063\n",
       "  ------                                  \n",
       "Overall AUC                       0.840792\n",
       "AUC difference                    0.019052"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metrics\n",
    "models_dict = {\"Unmitigated\": (test_preds, test_scores)}\n",
    "get_metrics_df(models_dict, Y_test, B_str_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925ea45a",
   "metadata": {},
   "source": [
    "## Mitigating Equalized Odds Difference with Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31b7138",
   "metadata": {},
   "source": [
    "We attempt to mitigate the disparities in the `lightgbm` predictions using the Fairlearn postprocessing algorithm `ThresholdOptimizer`. This algorithm finds a suitable threshold for the scores (class probabilities) produced by the `lightgbm` model by optimizing the accuracy rate under the constraint that the equalized odds difference (on training data) is zero. Since our goal is to optimize balanced accuracy, we resample the training data to have the same number of positive and negative examples. This means that `ThresholdOptimizer` is effectively optimizing balanced accuracy on the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b73f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess_est = ThresholdOptimizer(\n",
    "    estimator=model,\n",
    "    constraints=\"equalized_odds\",\n",
    "    prefit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36495216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced data set is obtained by sampling the same number of points from the majority class (Y=0)\n",
    "# as there are points in the minority class (Y=1)\n",
    "balanced_idx1 = df_train[Y_train==1].index\n",
    "pp_train_idx = balanced_idx1.union(Y_train[Y_train==0].sample(n=balanced_idx1.size, random_state=1234, replace=True).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "375f41d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_balanced = df_train.loc[pp_train_idx, :]\n",
    "Y_train_balanced = Y_train.loc[pp_train_idx]\n",
    "B_train_balanced = B_train.loc[pp_train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec0d9777",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\postprocessing\\_threshold_optimizer.py:270: FutureWarning: 'predict_method' default value is changed from 'predict' to 'auto'. Explicitly pass `predict_method='predict' to replicate the old behavior, or pass `predict_method='auto' or other valid values to silence this warning.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ThresholdOptimizer(constraints='equalized_odds',\n",
       "                   estimator=LGBMClassifier(learning_rate=0.03, max_depth=3,\n",
       "                                            metric='auc', num_leaves=10,\n",
       "                                            objective='binary'),\n",
       "                   prefit=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postprocess_est.fit(df_train_balanced, Y_train_balanced, sensitive_features=B_train_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7acd40a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess_preds = postprocess_est.predict(df_test, sensitive_features=B_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5426f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unmitigated</th>\n",
       "      <th>ThresholdOptimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Overall selection rate</th>\n",
       "      <td>0.732082</td>\n",
       "      <td>0.888586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demographic parity difference</th>\n",
       "      <td>0.007506</td>\n",
       "      <td>0.000878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demographic parity ratio</th>\n",
       "      <td>0.989798</td>\n",
       "      <td>0.999012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall balanced error rate</th>\n",
       "      <td>0.766138</td>\n",
       "      <td>0.692568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balanced error rate difference</th>\n",
       "      <td>0.011549</td>\n",
       "      <td>0.012815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True positive rate</th>\n",
       "      <td>0.857912</td>\n",
       "      <td>0.979632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False positive rate difference</th>\n",
       "      <td>0.020063</td>\n",
       "      <td>0.022357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False negative rate difference</th>\n",
       "      <td>0.003035</td>\n",
       "      <td>0.003272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equalized odds difference</th>\n",
       "      <td>0.020063</td>\n",
       "      <td>0.022357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall AUC</th>\n",
       "      <td>0.840792</td>\n",
       "      <td>0.692568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC difference</th>\n",
       "      <td>0.019052</td>\n",
       "      <td>0.012815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Unmitigated ThresholdOptimizer\n",
       "Overall selection rate            0.732082           0.888586\n",
       "Demographic parity difference     0.007506           0.000878\n",
       "Demographic parity ratio          0.989798           0.999012\n",
       "------                                                       \n",
       "Overall balanced error rate       0.766138           0.692568\n",
       "Balanced error rate difference    0.011549           0.012815\n",
       " ------                                                      \n",
       "True positive rate                0.857912           0.979632\n",
       "False positive rate difference    0.020063           0.022357\n",
       "False negative rate difference    0.003035           0.003272\n",
       "Equalized odds difference         0.020063           0.022357\n",
       "  ------                                                     \n",
       "Overall AUC                       0.840792           0.692568\n",
       "AUC difference                    0.019052           0.012815"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_dict = {\"Unmitigated\": (test_preds, test_scores),\n",
    "              \"ThresholdOptimizer\": (postprocess_preds, postprocess_preds)}\n",
    "get_metrics_df(models_dict, Y_test, B_str_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df7bfca",
   "metadata": {},
   "source": [
    "The `ThresholdOptimizer` algorithm significantly reduces the disparity according to multiple metrics. However, the performance metrics (balanced error rate as well as AUC) get worse. Before deploying such a model in practice, it would be important to examine in more detail why we observe such a sharp trade-off. In our case it is because the available features are much less informative for one of the demographic groups than for the other.\n",
    "\n",
    "Note that unlike the unmitigated model, `ThresholdOptimizer` produces 0/1 predictions, so its balanced error rate difference is equal to the AUC difference, and its overall balanced error rate is equal to 1 - overall AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce0cedf",
   "metadata": {},
   "source": [
    "## Mitigating Equalized Odds Difference with GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83cbf8d",
   "metadata": {},
   "source": [
    "We now attempt to mitigate disparities using the `GridSearch` algorithm. Unlike `ThresholdOptimizer`, the predictors produced by `GridSearch` do not access the sensitive feature at test time. Also, rather than training a single model, we train multiple models corresponding to different trade-off points between the performance metric (balanced accuracy) and fairness metric (equalized odds difference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4e8eae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    }
   ],
   "source": [
    "# Train GridSearch\n",
    "sweep = GridSearch(model,\n",
    "                   constraints=EqualizedOdds(),\n",
    "                   grid_size=50,\n",
    "                   grid_limit=3)\n",
    "\n",
    "sweep.fit(df_train_balanced, Y_train_balanced, sensitive_features=B_train_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a06d3278",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_preds = [predictor.predict(df_test) for predictor in sweep.predictors_] \n",
    "sweep_scores = [predictor.predict_proba(df_test)[:, 1] for predictor in sweep.predictors_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a88a663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized_odds_sweep = [\n",
    "    equalized_odds_difference(Y_test, preds, sensitive_features=B_str_test)\n",
    "    for preds in sweep_preds\n",
    "]\n",
    "balanced_accuracy_sweep = [balanced_accuracy_score(Y_test, preds) for preds in sweep_preds]\n",
    "auc_sweep = [roc_auc_score(Y_test, scores) for scores in sweep_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d472d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only non-dominated models (with respect to balanced accuracy and equalized odds difference)\n",
    "all_results = pd.DataFrame(\n",
    "    {\"predictor\": sweep.predictors_, \"accuracy\": balanced_accuracy_sweep, \"disparity\": equalized_odds_sweep}\n",
    ") \n",
    "non_dominated = [] \n",
    "for row in all_results.itertuples(): \n",
    "    accuracy_for_lower_or_eq_disparity = all_results[\"accuracy\"][all_results[\"disparity\"] <= row.disparity] \n",
    "    if row.accuracy >= accuracy_for_lower_or_eq_disparity.max(): \n",
    "        non_dominated.append(True)\n",
    "    else:\n",
    "        non_dominated.append(False)\n",
    "\n",
    "equalized_odds_sweep_non_dominated = np.asarray(equalized_odds_sweep)[non_dominated]\n",
    "balanced_accuracy_non_dominated = np.asarray(balanced_accuracy_sweep)[non_dominated]\n",
    "auc_non_dominated = np.asarray(auc_sweep)[non_dominated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dd360545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot equalized odds difference vs balanced accuracy\n",
    "plt.scatter(balanced_accuracy_non_dominated, equalized_odds_sweep_non_dominated, label=\"GridSearch Models\")\n",
    "plt.scatter(balanced_accuracy_score(Y_test, test_preds),\n",
    "            equalized_odds_difference(Y_test, test_preds, sensitive_features=B_str_test), \n",
    "            label=\"Unmitigated Model\")\n",
    "plt.scatter(balanced_accuracy_score(Y_test, postprocess_preds), \n",
    "            equalized_odds_difference(Y_test, postprocess_preds, sensitive_features=B_str_test),\n",
    "            label=\"ThresholdOptimizer Model\")\n",
    "plt.xlabel(\"Balanced Accuracy\")\n",
    "plt.ylabel(\"Equalized Odds Difference\")\n",
    "plt.legend(bbox_to_anchor=(1.55, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63572a1",
   "metadata": {},
   "source": [
    "As intended, `GridSearch` models appear along the trade-off curve between the large balanced accuracy (but also large disparity), and low disparity (but worse balanced accuracy). This gives the data scientist a flexibility to select a model that fits the application context best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b715dc10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAEGCAYAAACJhvrnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA7JUlEQVR4nO3deXgUVdYG8PdkIwkJJIGwhR2yA2GJYR9kk6CyuKAsIjPKKDgOzLiiojKKCqKfTkYRZURFFEVUFGQAdQRFBQkgkJ3ImhBIWBISCCHL+f7oaqaJnU4H6HRCv7/nqSddt+6tPhVo+lD31r2iqiAiIiJyRW7ODoCIiIjIWZgIERERkctiIkREREQui4kQERERuSwmQkREROSyPJwdQG1o2rSptm/f3tlhEBHVK9u3bz+uqsHOjoPIkVwiEWrfvj0SExOdHQYRUb0iIgedHQORo7FrjIiIiFwWEyEiIiJyWUyEiIiIyGW5xBghIiJyvO3btzfz8PD4N4Au4H+0qW6oAJBUVlY2tVevXrnWKjARIiKiK8LDw+PfLVq0iAwODj7l5ubGhSzJ6SoqKiQvLy/q6NGj/wYw2lodJkJERPXYqp3ZWLA+HUfyi9EqwAcPjwjH2B4hzgqnC5Mgqkvc3Nw0ODi44OjRo12qqsNEiIionlq1MxuPfbYHxaXlAIDs/GI89tkeAHBWMuTGJIjqGuPvZJVdtezDJSKqpxasT7+QBJkVl5Zjwfp0J0VEVP8wESIiqqeO5BfXqNwVHD582GPUqFEdWrdu3TU6Ojqye/fuEUuXLg2wVvfAgQOe8fHxHa0di4uLC//+++99AeDVV19tEhYWFhUWFhYVGhoavWzZMqvnuxLS09O9QkNDo6ur98ADD7QSkV5JSUkNzGX/+Mc/molIL3Pc9khISGhy5513tr3cOvUZEyEionqqVYBPjcqvdhUVFRg1alTngQMHFmVlZe1JTk5OXbFixb7Dhw97Va5bWlqK9u3bl65bt26frXP+9ttvni+//HLLn3/+OT0jIyMlMTExNTY29uzlxlpaWnq5p0BoaGjx0qVLg8z7X3zxRVCnTp3OXfaJXYxDEyERiReRdBHJFJFZVo6LiCQYx3eLSE+j3FtEfhGRXSKSLCL/sGgzR0SyReRXY7vekddARFRXPTwiHD6e7heV+Xi64+ER4U6KqGaWbTkYFPfcN107zPqqV9xz33RdtuVgUPWtqrZ69Wp/T09PfeSRR/LMZWFhYeefeOKJXMB0Z2PkyJEdhwwZ0nngwIFhlndfioqK5MYbb+wYFhYWdcMNN3Q8d+6cAEBOTo5nw4YNKxo3blwOAI0bN66IiIg4DwDJyckNBg4cGBodHR3Zq1ev8J07d3oDwIcffti4W7duEZGRkVH9+vULO3z4sAdguoszYcKEdv379w+9+eabOxw+fNhj+PDhncLDw6PCw8Ojvv7664YAUF5ejvHjx7fr3LlzdP/+/UOLiorE2vVef/31+WvXrg0AgJSUFC9/f/+yoKCgMvPxN998M8h8F2v69OkXBo3985//bNK+ffsu11xzTfhPP/3kZy4/cuSIx4gRIzp16dIlskuXLpEbNmxoWPk9lyxZEhgaGhodHh4eFRsbWz/+olXDYYmQiLgDeB3ASABRACaISFSlaiMBhBrbPQDeMMpLAAxR1RgA3QHEi0gfi3avqGp3Y1vrqGsgIqrLxvYIwQs3d0VIgA8EQEiAD164uasznxqz27ItB4OeXZPSLrewxEsB5BaWeD27JqXd5SRDe/bs8enWrZvNuzU7duzwW758+f4tW7ZkWJa/9NJLzXx8fCoyMjJSnnrqqZyUlJSGANCnT5+zTZs2LW3Tpk3XW2+9tf2HH37Y2Nxm6tSp7RYuXHgoOTk5dcGCBVnTp09vCwDDhw8v+vXXX9NSU1NTbr311pPPPPNMC3Ob3bt3+65fvz5z9erV+6dNm9Z24MCBhenp6SnJyckpPXv2PAcAhw4d8p4xY0ZuZmZmcuPGjcuXLl0aaO1aGjVqVN6qVavz27Zt837vvfeCbr311lPmYwcOHPCcM2dOyMaNGzNSUlKSd+7c2fD9998POHjwoOe8efNa/fTTT2k//PBDRkZGxoXbh/fee2+bBx544FhSUlLq559//tu0adPaV37PefPmtdywYUNGenp6yrp16zKr+SOpFxz51FgcgExV3QcAIvIRgDEAUizqjAGwVFUVwBYRCRCRlqqaA6DIqONpbHwSgYiokrE9QupF4lNZwrd7Q0rKKi76z3hJWYVbwrd7Q+7o0+7klXiPyZMnt/3ll1/8PD09NSkpKRUABg4ceLp58+blletu3rzZb8aMGbkA0Lt37+KwsLCzAODh4YHvv/9+76ZNm3w3bNjQaNasWW0SExMbPv3000d37tzpN27cuE7mc5w/f14AYP/+/V5jx45tnZeX53n+/Hm3Nm3alJjrxMfH5/v5+SkA/PTTT/4rV67cb36fJk2alB8/ftw9JCSkpF+/fsUA0KNHj7MHDhxogCrcdtttJ99///2g//73v42///779Pfff7+pcT0N+/TpU9iqVasyALj99ttPbtq0yQ8ALMtvvvnmkxkZGd4A8OOPPzbau3fvhcSoqKjI/dSpUxf9GcXGxhZNmjSp/S233HJq0qRJp3AVcGTXWAiAwxb7WUaZXXVExF1EfgWQC+BrVd1qUe9+oyttiYhYzZRF5B4RSRSRxLy8PGtViIjISfIKS343bsdWuT26du1avHv37gsDhd9///1DGzduzDh16tSF//T7+vpWVNVexGoPFNzc3DB48OCzL7zwwtFly5btW7NmTUB5eTn8/f3L0tLSUszbvn37kgHg/vvvb3vfffflZmRkpLz22msHS0pKLnzXNmzYsMr3N/Py8rrwH393d3ctKyuzHhiA8ePH569cubJJSEjI+aCgoAvnNt1fqNl1qioSExNTzdeTm5u7OzAw8KJ4P/zww0Nz5849cvjwYa/u3btHHz161N3qyeoRRyZC1n7Tlf9kqqyjquWq2h1AawBxImKeDOkNAJ1g6jLLAfCytTdX1bdUNVZVY4ODg2sePREROUywf4PzNSm3x6hRowpLSkpk/vz5F/7RLyoqsut7bsCAAUXLli0LAoBt27Z5Z2Rk+AKmLqbNmzdfSK4SExN9zUlH69atzy9ZsiQQMA3U/vnnn30AoLCw0L1t27alAPDuu+82qeo9+/fvX7hgwYJgACgrK8PJkydr/J3s5+enc+bMyXryySdzLMv/8Ic/nNm6dat/Tk6OR1lZGT755JOga6+9tugPf/jDmS1btvgfPXrUvaSkRD7//PMLNxMGDBhwev78+c3M+z/99NPvRt0nJyc3GDJkyJlXX331SGBgYNm+ffsuOXGtKxzZNZYFoI3FfmsAR2paR1XzRWQjgHgASap6zHxMRBYDWHMFYyYiolowY2ho9rNrUtpZdo818HCrmDE0NPtSz+nm5obVq1f/9pe//KVNQkJCi6CgoDJfX9/yOXPmZFXX9qGHHsodP358h7CwsKjo6OizXbt2PQOYurseeuih1seOHfNs0KCBBgUFlS5evPgQACxfvnzfn//853bz589vWVZWJjfddNPJvn37Fj/xxBNHJkyY0Kl58+bnY2Njzxw6dMhq19Ybb7xx6I9//GO7sLCwpm5ubnjttdcOtmnTpsaPk91zzz2/66Jq165d6VNPPZU9aNCgMFWVoUOHFtxxxx35APDoo48e6dOnT2RwcHBpt27dzpaXlwsAvPXWW4enTp3aNiwsLKq8vFx69+5d2K9fv0OW5/373//e+sCBAw1UVQYMGHC6T58+9X6uBrF1++yyTiziASADwFAA2QC2AZioqskWdW4AcD+A6wH0BpCgqnEiEgyg1EiCfABsADBfVddYjCGCiPwdQG9VHW8rltjYWE1MTHTAVRIRXb1EZLuqxtpbf9euXQdiYmKO21t/2ZaDQQnf7g3JKyzxCvZvcH7G0NDsKzU+iMjSrl27msbExLS3dsxhd4RUtUxE7gewHoA7gCWqmiwi04zjiwCshSkJygRwFsCfjOYtAbxnPHnmBmCFqprv/LwoIt1h6kI7AOBeR10DERE5zh192p1k4kPO5tC1xoxH29dWKltk8VoB/MVKu90AelRxzslXOEwiIiJyUZxZmoiIiFwWEyEiIiJyWUyEiIiIyGUxESIiIiKXxUSIiIiuCpaLqJo98MADrZ566qnmV+L8f/vb31qtWrXKHwCeeeaZZoWFhRe+QwcNGtT5+PHjlzTL8vvvvx+wfft275q28/X1tfpQkYj0Gjt2bAfzfmlpKQIDA2MGDx7cuSbnDwkJ6ZqTk2PzoSp76tR1TISIiIjs8Oqrrx4ZO3ZsIQC8+eabzS1nrd60aVNm06ZNf7eGmT1WrVoVsHv37t/N4nypfHx8KtLT033Mq9Z//vnnjZo3b17jiRpdBRMhIiJyjm1vB+GlsK6YE9ALL4V1xba3L3nleXvExcWFT58+PaRr166R7du377Ju3To/AEhISGgybNiwTkOGDOkcEhLS9fnnnw+eM2dO88jIyKiYmJiIY8eOuQPALbfc0v6dd94JnDt3brPc3FzPQYMGhfXu3TsMuPjOyMMPP9yyQ4cO0f369QsdNWpUB/MdqZdffrlply5dIsPDw6NGjBjRqbCw0O3rr79u+M033wTMnj27dURERFRycnKD5OTkBgMHDgyNjo6O7NWrV/jOnTu9ASAtLc2re/fuEV26dImcOXNmK1vXOnTo0IJPPvkkAACWL18edMstt1yYr+nYsWPuw4YN6xQWFhYVExMTsXXrVh8AOHr0qHv//v1DIyMjoyZOnNjOcsLlhQsXBnXt2jUyIiIiauLEie3Kysqu3B+MkzERIiKi2rft7SCsf6wdio55AQoUHfPC+sfaOToZKisrkz179qTOnz//8DPPPHMhmcjIyPD59NNP923bti31hRdeCPH19a1ITU1NiY2NPfPmm29etF7Y7Nmzc5s1a1a6adOmjK1bt2ZYHvv+++99V69eHbhnz56Ur7766rfdu3c3NB+bNGnSqaSkpNT09PSU8PDw4oSEhKbDhw8/M2zYsPy5c+dmpaWlpURHR5dMnTq13cKFCw8lJyenLliwIGv69OltAeC+++5rO3Xq1LykpKTUFi1a2LzDM3ny5JMff/xx4NmzZyU1NdW3b9++Z8zHHnnkkVYxMTFnMzIyUp599tnsKVOmdACAWbNmterbt29RampqyujRo/NzcnK8AGDHjh3eK1euDEpMTExLS0tLcXNz00WLFlW5hlp9w0SIiIhq36b5ISgrufg7qKzEDZvmh1zqKataVd2yfNy4cacAoF+/fmeysrIuLBjar1+/wsDAwIpWrVqV+fn5lY8bNy4fALp27Xr2wIEDVtcKs2bjxo1+I0eOzPfz89PAwMCK4cOH55uPbd++3adXr17hYWFhUZ9++mmT5OTk340LKigocNu5c6ffuHHjOkVERETdd9997XJzcz0BYMeOHX5//vOfTwLAvffee8JWHL179y7OyspqsHjx4qBhw4YVWB775Zdf/O++++4TADB69OjC/Px8jxMnTrhv2bLF/6677joBAOPHjy9o1KhROQCsW7fOPykpyTcmJiYyIiIiavPmzY327dtn9++krqvXA5yIiKieKsq1vmp5VeV2aN68eVlBQcFFA5ZPnjzp3qFDhxLzvre3twKAh4cHzIuNAoCXl9eFfiA3N7cL9dzc3FBWVmY9w7LC1vqd99xzT4eVK1dm9u3btzghIaHJpk2b/CvXKS8vh7+/f1laWlqKtXO4ubnZvUBofHx8/tNPP91mw4YN6bm5uRe+763FKCIXrrcyVZVx48adeP311y95Qdy6jHeEiIio9vk1O1+jcjs0bty4olmzZqVffPGFP2AaC7Nx48bGQ4YMKbrUc1alYcOG5QUFBb/7Dr322muL1q9f3/js2bNSUFDg9s033wSYj509e9atbdu2pSUlJfLRRx9d6AL08/MrP336tBsABAUFVbRu3fr8kiVLAgGgoqICP//8sw8A9OzZs2jx4sVBALB48eJqu6amT59+/MEHHzwSFxd30Qrxffr0KXznnXeaAMCaNWv8AwMDy4KCgir69OlTuGTJkiYAsGLFikanT592B4D4+PjTa9asCczOzvYATL/XjIyMS05Y6xomQkREVPsGPZoNjwYVF5V5NKjAoEcv667De++9t//5559vGRERETVo0KDwRx999Eh0dHRJ9S1rZsqUKcdHjhwZah4sbTZo0KCz8fHxBVFRUdHXX399p27dup1p3LhxOQDMmjXrSFxcXOTAgQPDQkNDz5nbTJo06WRCQkKLyMjIqOTk5AbLly/f98477zQNDw+PCg0Njf70008DAGDhwoWH3nrrrWZdunSJrHzny5pOnTqVPvnkk7mVy+fPn39kx44dvmFhYVFPPPFEyLvvvrsfAObNm3fkxx9/9IuKiopcv35945YtW54HgF69ep2bPXt29tChQ8PCwsKihgwZEnb48GHPy/oF1iFi6zbe1SI2NlYTExOdHQYRUb0iIttVNdbe+rt27ToQExNz3O432PZ2EDbND0FRrhf8mp3HoEezcc3d9X41+oKCArfGjRtXFBYWuvXt2zd80aJFBwcMGHDW2XG5sl27djWNiYlpb+0YxwgREZFzXHP3yash8ansjjvuaLd3716fkpISGT9+/AkmQXUbEyEiIqIraPXq1fudHQPZj2OEiIiIyGUxESIiIiKXxUSIiIiIXBYTISIiInJZTISIiOiqcPToUfeIiIioiIiIqKZNm8Y0a9asW0RERJS/v3/3Tp06RV/p93vggQdamRdUtZevr28Pa+XmBV0B4Ny5c3LXXXe1adOmTZd27dp1GTp0aKfffvut2nl7nnnmmWaFhYUXvtcHDRrU+fjx49XON2T2wQcfNH788cdb2FvfXiLSa+zYsR3M+6WlpQgMDIwZPHhw55qcx3Jh28upUxkTISIiuiq0aNGiPC0tLSUtLS3lzjvvzJs2bdqxtLS0lMTExBRrS0dUVlpqcx3TWjNjxoyQoqIit/379ycdPHgwafTo0fljx47tXFFRYbPdm2++2byoqOjChW7atCmzadOm5fa+76RJkwqef/75o5cRutXfoY+PT0V6erpPUVGRAMDnn3/eqHnz5nXjlw0mQkRE5CQfp38cNHjF4K7d3uvWa/CKwV0/Tv/YYSvPl5eXY/z48e06d+4c3b9//1Dzl3JcXFz4/fffH3LNNdeEz507t/kPP/zge80114RHR0dHDhgwIPTgwYOeADB37txmnTp1ig4LC4u68cYbO5rPm5qa6hMXFxfeunXrrnPnzm1mLp8zZ07z0NDQ6NDQ0OhnnnmmWeV4KioqcOedd7bt1KlT9LXXXtv5+PHjHgBQWFjotmLFiqaLFi067OFhurExc+bME15eXhWrV6/2T09P9+rQoUP0zTff3D4sLCwqPj6+Y2FhodvcuXOb5ebmeg4aNCjMPNu1+e6Iuc3tt9/eLjQ0NHr06NEdVq1a5d+zZ8+Idu3adfnuu+98ASAhIaHJnXfe2RYAzHfWIiIiory9vXt+9dVXfqdPn3YbN25c+y5dukRGRkZGLVu2LMDcbuTIkR2HDBnSeeDAgWGVrxUAhg4dWvDJJ58EAMDy5cuDbrnllgvzRx07dsx92LBhncLCwqJiYmIitm7d6gOY7vD1798/NDIyMmrixIntLCeAXrhwYVDXrl0jIyIioiZOnNiurKys5n8pDEyEiIio1n2c/nHQi9tebHe8+LiXQnG8+LjXi9tebOeoZOjQoUPeM2bMyM3MzExu3Lhx+dKlSwPNx/Lz8923bduW/vjjj+fOmDGj7RdffPFbcnJy6pQpU44/9NBDIQCQkJDQIikpKSUjIyPl3XffPWhum5mZ6b1p06aMbdu2pb700kutSkpK5IcffvD98MMPm2zfvj01MTExdenSpcE//vijj2U877//fkBmZmaD9PT05Hfffffgjh07/AAgJSWlQcuWLc8HBQVddPune/fuZ/fs2eMDAAcOHPCeNm1aXkZGRoq/v3/FggULgmfPnp3brFmz0k2bNmVs3bo1o/L1Hz582PvBBx/MTUtLS/7tt9+8P/jggyaJiYlpzz33XNZzzz3XsnJ98521p556Kjs6OvrMsGHDzjz++OMtBw8efDopKSn1hx9+SJ89e3Zr8xppO3bs8Fu+fPn+LVu2/O69AWDy5MknP/7448CzZ89Kamqqb9++fc+Yjz3yyCOtYmJizmZkZKQ8++yz2VOmTOkAALNmzWrVt2/fotTU1JTRo0fn5+TkeBnv5b1y5cqgxMTEtLS0tBQ3NzddtGhRtWuvVcWhiZCIxItIuohkisgsK8dFRBKM47tFpKdR7i0iv4jILhFJFpF/WLQJEpGvRWSv8TOw8nmJiKhuW7RrUcj58vMXfQedLz/vtmjXohBHvF9ISEhJv379igGgR48eZw8cONDAfGzChAknAWD37t0N9u7d6zNkyJCwiIiIqAULFrQ8cuSIJwCEh4cX33TTTR0WLlwY5OnpeeHWxHXXXZfv4+OjLVu2LAsKCirNysry2Lhxo9/111+f36hRo4rGjRtX3HDDDae+++67i1aa37Rpk/9tt9120sPDA+3bty/t27dvIWC6U2ReCd6SqkJEAAAtWrQ4f911150BgMmTJ5/46aef/Oy5/ri4uGJ3d3eEhYUVDxky5LSbmxt69ux5Nisrq4G1Nnv27GnwxBNPtP7000/3NWjQQDdu3NjolVdeaRkRERE1YMCA8JKSEsnMzPQCgIEDB55u3rx5ld1wvXv3Ls7KymqwePHioGHDhhVYHvvll1/877777hMAMHr06ML8/HyPEydOuG/ZssX/rrvuOgEA48ePL2jUqFE5AKxbt84/KSnJNyYmJjIiIiJq8+bNjfbt22f1GuxR7YAiEQkD8AaA5qraRUS6ARitqnOraecO4HUAwwFkAdgmIl+qaopFtZEAQo2tt/E+vQGUABiiqkUi4glgs4j8R1W3AJgF4FtVnWckV7MAPFqzyyYiImc6UXzC6urlVZVfLi8vrwvJhbu7uxYXF19Iwvz9/SsAQFWlc+fOxb/++mta5fbffffd3v/85z/+q1atCnjxxRdb7d27NwkAGjRoYHlelJWV2b2GpzmxsRQdHV1y5MiRBqdOnXILDAy8cFdo9+7dvmPGjMm31s7aeSqzvH43Nzd4e3urOeby8vLfneD06dNut912W6c33njjYPv27UsBUzK2cuXKzJiYmIsWsd28eXNDX19f2wOYAMTHx+c//fTTbTZs2JCem5t7If+w9vsyJ4PWxnapqowbN+7E66+/flkL9JrZc0doMYDHAJh/EbsBjLejXRyATFXdp6rnAXwEYEylOmMALFWTLQACRKSlsV9k1PE0NrVo857x+j0AY+2IhYiI6pAmPk3O16S8NnTr1u3cyZMnPb755puGAFBSUiKJiYne5eXl+O2337xGjRpVuHDhwqzCwkJ3W6u/DxkypGjt2rUBhYWFbqdPn3Zbu3Zt4ODBgwst6wwaNKjwk08+CSorK8PBgwc9t2zZ4g8AjRo1qrj11luPT58+vY153Mtrr73W5Ny5c26jRo0qBICcnBwvc4wffvhhUL9+/YoAoGHDhuUFBQVXpKdn/Pjx7SdNmnQ8Pj7e/F2MwYMHn3755ZebmwdtV+7uq8706dOPP/jgg0fi4uKKLcv79OlT+M477zQBgDVr1vgHBgaWBQUFVfTp06dwyZIlTQBgxYoVjU6fPu0OAPHx8afXrFkTmJ2d7QGYxhhlZGRccgJtzy/MV1V/qVRmz6ikEACHLfazjDK76oiIu4j8CiAXwNequtWo01xVcwDA+Pm7QWhG+3tEJFFEEvPy8uwIl4iIasu0mGnZXu5eF91F8HL3qpgWM+2K/C//Unh7e+tHH33026xZs1qHh4dHRUdHR23atMmvrKxMJk6c2CEsLCyqS5cuUffee+8xW09jDRgw4OzEiRNP9OzZM7JXr16RkydPzuvfv/9FX/6TJ0/O79ixY0l4eHj03Xff3TYuLu5CovSvf/0ru0GDBhUdOnTo0q5duy6fffZZ4KpVqzLNd0c6dux4bsmSJU3CwsKiTp065fHQQw/lAcCUKVOOjxw5MtQ8WPpSZWRkeK1bty5w2bJlTc0Dpr///nvfefPmHSkrK5OIiIio0NDQ6NmzZ9eoG7NTp06lTz75ZG7l8vnz5x/ZsWOHb1hYWNQTTzwR8u677+4HgHnz5h358ccf/aKioiLXr1/fuGXLlucBoFevXudmz56dPXTo0LCwsLCoIUOGhB0+fLja6QWqUu0tPBH5D4D7AXyiqj1F5FYAd6vqyGrajQMwQlWnGvuTAcSp6l8t6nwF4AVV3WzsfwvgEVXdblEnAMDnAP6qqkkikq+qARbHT6mqzXFCsbGxmpiYaPM6iYjoYiKyXVVj7a2/a9euAzExMcftrf9x+sdBi3YtCjlRfMKriU+T89NipmXfHn77Vbca/ZWUnp7udeONN4bu3bs32dmx1Ce7du1qGhMT097aMXsmHfoLgLcARIhINoD9AO6wo10WgDYW+60BHKlpHVXNF5GNAOIBJAE4ZnSf5YhIS5juGBERUT1ze/jtJ5n4kLNV2zVmjPEZBiAYQISqDlDVA3acexuAUBHpICJeMI0r+rJSnS8B3Gk8PdYHQIGR4AQbd4IgIj4AhgFIs2gzxXg9BcAXdsRCRERU74WHh5/n3aArq9pESESeF5EAVT2jqoUiEigiNp8YAwBVLYOpS209gFQAK1Q1WUSmicg0o9paAPsAZMI0KPs+o7wlgO9EZDdMCdXXqrrGODYPwHAR2QvTE2nz7L5aIiJypIqKiorqH2EiqkXG38kqn2qzp2tspKo+bt5R1VMicj2A2dU1VNW1MCU7lmWLLF4rTF1vldvtBmB1PRZVPQFgqB1xExFR7UrKy8uLCg4OLnBzc7PvGXIiB6qoqJC8vLzGMA2tscqeRMhdRBqoaglwoavqkicuIiKiq1NZWdnUo0eP/vvo0aNdwJULqG6oAJBUVlY2taoK9iRCywB8KyLvwDSXz1343zw+REREAIBevXrlAhjt7DiIaqLaREhVXxSRPTB1RwmAZ1V1vcMjIyIiInIwe+4IQVX/A+A/Do6FiIiIqFbZ89TYzcYCpwUiclpECkXkdG0ER0RERORI9twRehHAKFVNdXQwRERERLXJnlH9x5gEERER0dXInjtCiSLyMYBVAErMhar6maOCIiIiIqoN9iRCjQCcBXCdRZkCYCJERERE9Zo9j8//qTYCISIiIqpt9jw1FiYi34pIkrHfTUSqXV6DiIiIqK6zZ7D0YgCPASgFLqwDNt6RQRERERHVBnsSIV9V/aVSWZkjgiEiIiKqTfYkQsdFpBNMA6QhIrcCyHFoVERERES1wJ6nxv4C4C0AESKSDWA/gEkOjYqIiIioFthMhETEHcB0VR0mIg0BuKlqYe2ERkRERORYNhMhVS0XkV7G6zO1ExIRERFR7bCna2yniHwJ4BMAF5IhzixNRERE9Z09iVAQgBMAhliUcWZpIiIiqvc4szQRERG5LM4sTURERC6LM0sTERGRy+LM0kREROSyHDqztIjEi0i6iGSKyCwrx0VEEozju0Wkp1HeRkS+E5FUEUkWkZkWbeaISLaI/Gps19t1pURERESVOGxmaWMyxtcBDAeQBWCbiHypqikW1UYCCDW23gDeMH6WAXhQVXeIiD+A7SLytUXbV1T1JbuukIiIiKgKVd4RsrgL01JVhwEIBhChqgNU9aAd544DkKmq+1T1PICPAIypVGcMgKVqsgVAgIi0VNUcVd0BAMZM1qkAQmp2aURERES22eoaMz82/y/ANLN0DZfXCAFw2GI/C79PZqqtIyLtAfQAsNWi+H6jK22JiARae3MRuUdEEkUkMS8vrwZhExERkauwlQilisgBAOFG0mHe9ojIbjvOLVbKtCZ1RMQPwKcA/qaqp43iNwB0AtAdprFKL1t7c1V9S1VjVTU2ODjYjnCJiIjI1VQ5RkhVJ4hICwDrAYy+hHNnAWhjsd8awBF764iIJ0xJ0AeWy3mo6jHzaxFZDGDNJcRGREREZHOM0LeqehTAelU9WHmz49zbAISKSAcR8YJp7qEvK9X5EsCdxtNjfQAUqGqOiAiAtwGkqur/VYqrpcXuTQCS7IiFiIiI6HdsPTXWUkQGARglIstRqRvLPJi5KqpaJiL3w3RHyR3AElVNFpFpxvFFANYCuB5AJoCz+N+4pP4AJgPYIyK/GmWPq+paAC+KSHeYutAOALjXvkslIiIiupioVh62YxwwzRd0N4ABABIrHVZVHfL7VnVTbGysJiZWvgQiIrJFRLaraqyz4yByJFtjhFYCWCkiT6rqs7UYExEREVGtqDIREpEIVU0D8JV5xmdL1XWNEREREdV1tsYIPQDgHlh/PF0B1JuuMSIiIiJrbHWN3WP8HFx74RARERHVHptrjYlIEwATAUQYRakAPlTVk44OjIiIiMjRbM0jFAnTHD29AGQA2AvgGgBJIhJRVTsiIiKi+sLWHaFnAcxU1RWWhSJyC4DnANziyMCIiIiIHM3WWmNdKydBAKCqnwLo4riQiIiIiGqHrUTozCUeuyp8te8rXLfyOnR7rxuuW3kdvtr3lbNDIiIioivMVtdYMxF5wEq5ALiql3P/at9XmPPTHJwrPwcAyDmTgzk/zQEA3NDxBidGRkRERFeSrTtCiwH4W9n8APzb8aE5zz93/PNCEmR2rvwc/rnjn06KiIiIiBzB1jxC/6jNQOqSo2eO1qiciIiI6idbd4RcVouGLWpUTkRERPUTEyErZvacCW9374vKvN29MbPnTCdFRERERI5gc2ZpV2UeEP3PHf/E0TNH0aJhC8zsOZMDpYmIiK4y1SZCIjITwDsACmEaJN0DwCxV3eDg2Jzqho43MPEhIiK6ytnTNXaXqp4GcB1Mj83/CcA8h0ZFREREVAvsSYTE+Hk9gHdUdZdFGREREVG9ZU8itF1ENsCUCK0XEX8AFY4Ni4iIiMjx7BksfTeA7gD2qepZEWkCU/cYERERUb1WZSIkIj0rFXUUYY8YERERXT1s3RF62fjpDaAXgN0wjQ3qBmArgAGODY2IiOy1amc2FqxPx5H8YrQK8MHDI8IxtkeIs8MiqvOqHCOkqoNVdTCAgwB6qWqsqvaC6fH5zNoKkIiIbFu1MxuPfbYH2fnFUADZ+cV47LM9WLUz29mhEdV59gyWjlDVPeYdVU2CacxQtUQkXkTSRSRTRGZZOS4ikmAc323ujhORNiLynYikikiyMZeRuU2QiHwtInuNn4H2xEJEdLVasD4dxaXlF5UNL9+EPl8MAuYEAK90AXavcE5wRHWcPYlQqoj8W0SuFZFBIrIYQGp1jUTEHcDrAEYCiAIwQUSiKlUbCSDU2O4B8IZRXgbgQVWNBNAHwF8s2s4C8K2qhgL41tgnInJZR/KLL9of7bYZ8zz/jRbIA6BAwWFg9QwmQ0RW2JMI/QlAMoCZAP4GIAX2PTUWByBTVfep6nkAHwEYU6nOGABL1WQLgAARaamqOaq6AwBUtRCmxCvEos17xuv3AIy1IxYioqtWqwCfi/Yf8VgBXzl/caXSYuDbZ2oxKqL6odpESFXPqeorqnqTsb2iqufsOHcIgMMW+1n4XzJjdx0RaQ/TuKStRlFzVc0xYssB0Mzam4vIPSKSKCKJeXl5doRLRFQ/PTwiHD6e7hf2W8lx6xULsmopIqL6w9bj83sAaFXHVbVbNee29qx95fPZrCMifgA+BfA3Y5kPu6nqWwDeAoDY2Ngqr4OIqL4zPx1mfmosV4KNbrFKGreu5ciI6j5bj8/faPz8i/HzfePnJABn7Th3FoA2FvutARyxt46IeMKUBH2gqp9Z1Dlm7j4TkZYAcu2IhYjoqja2R8j/HpfffcY0JqjUYuyQpw8w9CnnBEdUh9l6fP6gqh4E0F9VH1HVPcY2C8AIO869DUCoiHQQES8A4wF8WanOlwDuNJ4e6wOgwEhwBMDbAFJV9f+stJlivJ4C4As7YiEich3dbgNGJQCN2wAQ089RCaZyIrqIPUtsNBSRAaq6GQBEpB+AhtU1UtUyEbkfwHoA7gCWqGqyiEwzji8CsBamNcwyYbrLZB6E3R/AZAB7RORXo+xxVV0L08r3K0TkbgCHAIyz60qJiFxJt9uY+BDZQVRtD58RkV4AlgBobBTlA7jL/FRXfRAbG6uJiYnODoOIqF4Rke2qGuvsOIgcqdo7Qqq6HUCMiDSCKXEqcHxYRERUFS6nQXTl2EyERKQLgEdgmhBRAaSIyEuWM00TEVHtMS+nYZ5J2rycBgAmQ0SXoMrB0iIyBsDnADYCuAvAVACbAHxmHCMiolpmbTmN4tJyLFif7qSIiOo3W3eEngEwXFUPWJTtEpH/wvSkFp/WIiKqZZWX06iunIhsszWztGelJAgAYJR5OiogIiKqWuXlNKorJyLbbCVCpSLStnKhiLSDaVFUIiKqZZWX0wAAH093PDwi3EkREdVvtrrGngbwjYg8D2A7TIOlr4FptfdHayE2IiKqpPJyGnxqjOjyVJkIqeoqEdkP4EEAf4VpXbAkALep6q5aio+IiCq5aDkNIrosNh+fNxKeO2spFiIiIqJaZWuMEBEREdFVjYkQERERuSwmQkREROSyqhwjJCL/gulJMatUdYZDIiIiIiKqJbbuCCXC9Ni8N4CeAPYaW3cA5VU3IyIiIqofbD0+/x4AiMgfAQxW1VJjfxGADbUSHREREZED2TNGqBUAf4t9P6OMiIiIqF6zOY+QYR6AnSLynbE/CMAch0VEREREVEuqTYRU9R0R+Q+A3kbRLFU96tiwiIiIiByv2kRIRATAMAAdVfUZEWkrInGq+ovjwyMioqqs2pnNNceILpM9Y4QWAugLYIKxXwjgdYdFRERE1Vq1MxuPfbYH2fnFUADZ+cV47LM9WLUz29mhEdUr9iRCvVX1LwDOAYCqngLg5dCoiIjIpgXr01FcevFMJsWl5ViwPt1JERHVT/YkQqUi4g5jckURCQZQ4dCoiIjIpiP5xTUqJyLr7EmEEgB8DqCZiDwHYDOA5x0aFRER2dQqwKdG5URkXbWJkKp+AOARAC8AyAEwVlU/sefkIhIvIukikikis6wcFxFJMI7vFpGeFseWiEiuiCRVajNHRLJF5Fdju96eWIiIriYPjwiHj6f7RWU+nu54eES4kyIiqp+qTYRE5G0A3qr6uqq+pqqpIjLHjnbuMA2qHgkgCsAEEYmqVG0kgFBjuwfAGxbH3gUQX8XpX1HV7sa2trpYiIiuNmN7hOCFm7siJMAHAiAkwAcv3NyVT40R1ZA9EyqOANBLRP5PVZcaZaNR/aSKcQAyVXUfAIjIRwDGAEixqDMGwFJVVQBbRCRARFqqao6qfi8i7WtwLURELmVsjxAmPkSXyZ4xQrkA/gBgnIi8LiIeAMSOdiEADlvsZxllNa1jzf1GV9oSEQm0VkFE7hGRRBFJzMvLs+OURERE5GrsSYREVU+r6igAeQA2AWhsTzsrZXoJdSp7A0AnAN1hGrP0srVKqvqWqsaqamxwcHA1pyQiIiJXZE8i9KX5harOgWnQ9AE72mUBaGOx3xrAkUuocxFVPaaq5apaAWAxTF1wRERERDVmz1NjT1faX6OqQ+w49zYAoSLSQUS8AIyHRVJl+BLAncbTY30AFKhqjq2TikhLi92bACRVVZeIiIjIlioHS4vIZlUdICKFuLi7SgCoqjaydWJVLROR+wGsB+AOYImqJovINOP4IgBrAVwPIBPAWQB/snj/5QCuBdBURLIAPK2qbwN4UUS6GzEdAHBvja6YiIiIyCCmB7aubrGxsZqYmOjsMIiI6hUR2a6qsc6Og8iRbN0RCrLVUFVPXvlwiIiIiGqPrXmEtsPU/VTVk10dHRIRERERUS2pMhFS1Q61GQgRERFRbbNnZmkYkxaGAvA2l6nq944KioiIiKg2VJsIichUADNhmuPnVwB9APwMwJ5H6ImIiIjqLHsmVJwJ4BoAB1V1MIAeMM0wTURERFSv2ZMInVPVcwAgIg1UNQ1AuGPDIiIiInI8e8YIZYlIAIBVAL4WkVOoZhkMIiIiovqg2kRIVW8yXs4Rke9gWnB1nUOjIiIiIqoF9gyWbmuxu9/42QLAIYdERERERFRL7Oka+wr/m1jRG0AHAOkAoh0YFxEREZHD2dM11tVyX0R6ggudEhER0VXAnqfGLqKqO2B6nJ6IiIioXrNnjNADFrtuAHqC8wgRERHRVcCeMUL+Fq/LYBoz9KljwiEiIiKqPfaMEfpHbQRCREREVNvs6Rr70tZxVR195cIhIiIiqj32dI3th2neoGXG/gQABwCsd1BMRERERLXCnkSoh6r+wWJ/tYh8r6qPOyooIiIiotpgz+PzwSLS0bwjIh0ABDsuJCIiIqLaYc8dob8D2Cgi+4z99uCEikRERHQVsOepsXUiEgogwihKU9USx4ZFRERE5HhVdo2JyCMWu6NVdZexlYjI87UQGxEREZFD2RojNN7i9WOVjsXbc3IRiReRdBHJFJFZVo6LiCQYx3cb65iZjy0RkVwRSarUJkhEvhaRvcbPQHtiISIiIqrMViIkVby2tv/7xiLuAF4HMBJAFIAJIhJVqdpIAKHGdg+ANyyOvQvrCdcsAN+qaiiAb419IiIiohqzlQhpFa+t7VsTByBTVfep6nkAHwEYU6nOGABL1WQLgAARaQkAqvo9gJNWzjsGwHvG6/cAjLUjFiIiIqLfsTVYOkZETsN098fHeA1j39uOc4cAOGyxnwWgtx11QgDk2Dhvc1XNAQBVzRGRZnbEQkRERPQ7VSZCqup+mee21n1W+U6SPXUu7c1F7oGpuw1t27a9EqckIiKiq4w9EypeqiwAbSz2WwM4cgl1Kjtm7j4zfuZaq6Sqb6lqrKrGBgdz/kciIiL6PUcmQtsAhIpIBxHxgukptMoLuH4J4E7j6bE+AArM3V42fAlgivF6CoAvrmTQRERE5DoclgipahmA+2FanDUVwApVTRaRaSIyzai2FsA+AJkAFgO4z9xeRJYD+BlAuIhkicjdxqF5AIaLyF4Aw419IiIiohoT1SsyJKdOi42N1cTERGeHQUQubNXObCxYn44j+cVoFeCDh0eEY2yPEGeHZZOIbFfVWGfHQeRI9qw1RkREl2HVzmw89tkeFJeWAwCy84vx2Gd7AKDOJ0NEVztHjhEiIiIAC9anX0iCzIpLy7FgfbqTIiIiMyZCREQOdiS/uEblRFR7mAgRETlYqwCfGpUTUe1hIkRE5GAPjwiHj+fFc9T6eLrj4RHhToqIiMw4WJqI6rz6+MSVJXOs9fkaiK5WTISIqE67Wp64GtsjpF7FS+Qq2DVGRHUan7giIkdiIkREdRqfuCIiR2IiRER1Gp+4IiJHYiJERHUan7giIkfiYGkiqtP4xBURORITISKq8/jEFRE5CrvGiIiIyGUxESIiIiKXxUSIiIiIXBYTISIiInJZTISIiIjIZTERIiIiIpfFRIiIiIhcFhMhIiIicllMhIiIiMhlMREiIiIil8VEiIiIiFyWQxMhEYkXkXQRyRSRWVaOi4gkGMd3i0jP6tqKyBwRyRaRX43tekdeAxHVHat2ZqP/vP+iw6yv0H/ef7FqZ7azQyKies5hi66KiDuA1wEMB5AFYJuIfKmqKRbVRgIINbbeAN4A0NuOtq+o6kuOip2I6p5VO7Px2Gd7UFxaDgDIzi/GY5/tAQAuyEpEl8yRd4TiAGSq6j5VPQ/gIwBjKtUZA2CpmmwBECAiLe1sS0QuZMH69AtJkFlxaTkWrE93UkREdDVwZCIUAuCwxX6WUWZPnera3m90pS0RkUBrby4i94hIoogk5uXlXeo1EFEdcSS/uEblRET2cGQiJFbK1M46ttq+AaATgO4AcgC8bO3NVfUtVY1V1djg4GC7AiaiuqtVgE+NyomI7OHIRCgLQBuL/dYAjthZp8q2qnpMVctVtQLAYpi60YjoKvfwiHD4eLpfVObj6Y6HR4Q7KSIiuho4MhHaBiBURDqIiBeA8QC+rFTnSwB3Gk+P9QFQoKo5ttoaY4jMbgKQ5MBrIKI6YmyPELxwc1eEBPhAAIQE+OCFm7tyoDQRXRaHPTWmqmUicj+A9QDcASxR1WQRmWYcXwRgLYDrAWQCOAvgT7baGqd+UUS6w9RVdgDAvY66BiKqW8b2CGHiQ0RXlKhWHrZz9YmNjdXExERnh0FEVK+IyHZVjXV2HESOxJmliYiIyGUxESIiIiKXxUSIiIiIXBYTISIiInJZLjFYWkTyABx0wls3BXDcCe9bE/UhRqB+xMkYrwzGeGVciRjbqSpnpKWrmkskQs4iIol1/YmL+hAjUD/iZIxXBmO8MupDjER1AbvGiIiIyGUxESIiIiKXxUTIsd5ydgB2qA8xAvUjTsZ4ZTDGK6M+xEjkdBwjRERERC6Ld4SIiIjIZTERIiIiIpfFROgSiUi8iKSLSKaIzLJy/GER+dXYkkSkXESC7Gnr7BhFpI2IfCciqSKSLCIz61qMFsfdRWSniKypizGKSICIrBSRNOP32bcOxvh34885SUSWi4i3k2JsLCKrRWSXEc+f7G3r7Bjr2Gemyt+jcdzhnxmiekVVudVwA+AO4DcAHQF4AdgFIMpG/VEA/nspbZ0UY0sAPY3X/gAy6lqMFmUPAPgQwJq69mdt7L8HYKrx2gtAQF2KEUAIgP0AfIz9FQD+6IwYATwOYL7xOhjASaNunfnM2IixznxmqorR4rhDPzPcuNW3jXeELk0cgExV3aeq5wF8BGCMjfoTACy/xLa1HqOq5qjqDuN1IYBUmL4w60yMACAirQHcAODfDojtsmMUkUYA/gDgbQBQ1fOqml+XYjR4APAREQ8AvgCOOClGBeAvIgLAD6Yv8DI72zo1xjr2manq91hbnxmieoWJ0KUJAXDYYj8LVfyjJyK+AOIBfFrTtk6M0fJYewA9AGy98iFedoyvAngEQIUDYjO7nBg7AsgD8I7RFfFvEWlYl2JU1WwALwE4BCAHQIGqbnBSjK8BiIQpEdsDYKaqVtjZ1tkxXlAHPjO2YnwVjv/MENUrTIQujVgpq2oeglEAflTVk5fQ9nJcToymE4j4wfSF+TdVPX2F4wMuI0YRuRFArqpud0Bcli7n9+gBoCeAN1S1B4AzABwxvuVyfo+BMN1R6ACgFYCGInKHk2IcAeBXI47uAF4z7qrVpc9MVTGaTlA3PjNWY6zFzwxRvcJE6NJkAWhjsd8aVXcnjMfF3RA1aXs5LidGiIgnTP+gf6CqnzkgPuDyYuwPYLSIHICpe2CIiCyrYzFmAchSVfOdgZUwJUZX2uXEOAzAflXNU9VSAJ8B6OekGP8E4DM1yYRp7FKEnW2dHWNd+sxUFWNtfWaI6hdnD1KqjxtM/9PfB9P/os0DFqOt1GsMU/98w5q2dXKMAmApgFfr6u+x0vFr4bjB0pcVI4AfAIQbr+cAWFCXYgTQG0AyTGODBKbB3X91RowA3gAwx3jdHEA2TCuo15nPjI0Y68xnpqoYK9Vx2GeGG7f6tnmAakxVy0TkfgDrYXqKY4mqJovINOP4IqPqTQA2qOqZ6trWpRhh+p/jZAB7RORXo+xxVV1bh2KsFVcgxr8C+EBEvGD6AvsTrrDL/Pu4VURWAtgB04DanXDA0gx2xvgsgHdFZA9MicWjqnocAOrQZ8ZqjCIyAHXnM1Pl75GIfo9LbBAREZHL4hghIiIicllMhIiIiMhlMREiIiIil8VEiIiIiFwWEyEiIiJyWUyEiBxIRG4SERUR86R711Ze9VtE3hWRW43XniIyT0T2GqvB/yIiI50ROxGRK2AiRORYEwBshmlGZ3s8C9NK5l1UtQtMS2L4Oyg2IiKXx0SIyEGMdaf6A7gbdiRCxoKof4ZpZucSAFDVY6q6wqGBEhG5MCZCRI4zFsA6Vc0AcFJEqltnrDOAQ+qYxTqJiMgKJkJEjjMBpsUtYfycgKpXTecU70RETsC1xogcQESaABgCoIuIKEzrQilMC3MGVqoeBOA4gEwAbUXEX1ULazNeIiJXxTtCRI5xK4ClqtpOVdurahsA+2FKelqJSCQAiEg7ADEAflXVswDeBpBgLNIKEWkpInc45xKIiK5+TISIHGMCgM8rlX0K06DpOwC8Y6xSvhLAVFUtMOrMBpAHIEVEkgCsMvaJiMgBuPo8ERERuSzeESIiIiKXxUSIiIiIXBYTISIiInJZTISIiIjIZTERIiIiIpfFRIiIiIhcFhMhIiIicln/D+lvSL6jh4gDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot equalized odds difference vs AUC\n",
    "plt.scatter(auc_non_dominated, equalized_odds_sweep_non_dominated, label=\"GridSearch Models\")\n",
    "plt.scatter(roc_auc_score(Y_test, test_scores),\n",
    "            equalized_odds_difference(Y_test, test_preds, sensitive_features=B_str_test), \n",
    "            label=\"Unmitigated Model\")\n",
    "plt.scatter(roc_auc_score(Y_test, postprocess_preds), \n",
    "            equalized_odds_difference(Y_test, postprocess_preds, sensitive_features=B_str_test),\n",
    "            label=\"ThresholdOptimizer Model\")\n",
    "plt.xlabel(\"AUC\")\n",
    "plt.ylabel(\"Equalized Odds Difference\")\n",
    "plt.legend(bbox_to_anchor=(1.55, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27670bd",
   "metadata": {},
   "source": [
    "Similarly, `GridSearch` models appear along the trade-off curve between AUC and equalized odds difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a6852f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unmitigated</th>\n",
       "      <th>ThresholdOptimizer</th>\n",
       "      <th>GridSearch_8</th>\n",
       "      <th>GridSearch_13</th>\n",
       "      <th>GridSearch_20</th>\n",
       "      <th>GridSearch_21</th>\n",
       "      <th>GridSearch_29</th>\n",
       "      <th>GridSearch_31</th>\n",
       "      <th>GridSearch_46</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Overall selection rate</th>\n",
       "      <td>0.732082</td>\n",
       "      <td>0.888586</td>\n",
       "      <td>0.734325</td>\n",
       "      <td>0.72266</td>\n",
       "      <td>0.734849</td>\n",
       "      <td>0.735746</td>\n",
       "      <td>0.735933</td>\n",
       "      <td>0.719894</td>\n",
       "      <td>0.697686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demographic parity difference</th>\n",
       "      <td>0.007506</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.005063</td>\n",
       "      <td>0.009791</td>\n",
       "      <td>0.003863</td>\n",
       "      <td>0.026939</td>\n",
       "      <td>0.006955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demographic parity ratio</th>\n",
       "      <td>0.989798</td>\n",
       "      <td>0.999012</td>\n",
       "      <td>0.999376</td>\n",
       "      <td>0.999048</td>\n",
       "      <td>0.993133</td>\n",
       "      <td>0.986777</td>\n",
       "      <td>0.994764</td>\n",
       "      <td>0.963242</td>\n",
       "      <td>0.990079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall balanced error rate</th>\n",
       "      <td>0.766138</td>\n",
       "      <td>0.692568</td>\n",
       "      <td>0.754869</td>\n",
       "      <td>0.763282</td>\n",
       "      <td>0.765257</td>\n",
       "      <td>0.766569</td>\n",
       "      <td>0.764828</td>\n",
       "      <td>0.767684</td>\n",
       "      <td>0.745479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balanced error rate difference</th>\n",
       "      <td>0.011549</td>\n",
       "      <td>0.012815</td>\n",
       "      <td>0.007552</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.012227</td>\n",
       "      <td>0.010204</td>\n",
       "      <td>0.011107</td>\n",
       "      <td>0.009077</td>\n",
       "      <td>0.001762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True positive rate</th>\n",
       "      <td>0.857912</td>\n",
       "      <td>0.979632</td>\n",
       "      <td>0.854828</td>\n",
       "      <td>0.847141</td>\n",
       "      <td>0.860262</td>\n",
       "      <td>0.86178</td>\n",
       "      <td>0.861144</td>\n",
       "      <td>0.846455</td>\n",
       "      <td>0.813749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False positive rate difference</th>\n",
       "      <td>0.020063</td>\n",
       "      <td>0.022357</td>\n",
       "      <td>0.007127</td>\n",
       "      <td>0.008828</td>\n",
       "      <td>0.018671</td>\n",
       "      <td>0.020287</td>\n",
       "      <td>0.01577</td>\n",
       "      <td>0.035695</td>\n",
       "      <td>0.004964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False negative rate difference</th>\n",
       "      <td>0.003035</td>\n",
       "      <td>0.003272</td>\n",
       "      <td>0.007976</td>\n",
       "      <td>0.008413</td>\n",
       "      <td>0.005782</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.006444</td>\n",
       "      <td>0.017541</td>\n",
       "      <td>0.001441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equalized odds difference</th>\n",
       "      <td>0.020063</td>\n",
       "      <td>0.022357</td>\n",
       "      <td>0.007976</td>\n",
       "      <td>0.008828</td>\n",
       "      <td>0.018671</td>\n",
       "      <td>0.020287</td>\n",
       "      <td>0.01577</td>\n",
       "      <td>0.035695</td>\n",
       "      <td>0.004964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall AUC</th>\n",
       "      <td>0.840792</td>\n",
       "      <td>0.692568</td>\n",
       "      <td>0.803496</td>\n",
       "      <td>0.827145</td>\n",
       "      <td>0.836825</td>\n",
       "      <td>0.83926</td>\n",
       "      <td>0.837586</td>\n",
       "      <td>0.837864</td>\n",
       "      <td>0.804387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC difference</th>\n",
       "      <td>0.019052</td>\n",
       "      <td>0.012815</td>\n",
       "      <td>0.026507</td>\n",
       "      <td>0.022811</td>\n",
       "      <td>0.020177</td>\n",
       "      <td>0.019253</td>\n",
       "      <td>0.017592</td>\n",
       "      <td>0.016539</td>\n",
       "      <td>0.005434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Unmitigated ThresholdOptimizer GridSearch_8  \\\n",
       "Overall selection rate            0.732082           0.888586     0.734325   \n",
       "Demographic parity difference     0.007506           0.000878     0.000459   \n",
       "Demographic parity ratio          0.989798           0.999012     0.999376   \n",
       "------                                                                       \n",
       "Overall balanced error rate       0.766138           0.692568     0.754869   \n",
       "Balanced error rate difference    0.011549           0.012815     0.007552   \n",
       " ------                                                                      \n",
       "True positive rate                0.857912           0.979632     0.854828   \n",
       "False positive rate difference    0.020063           0.022357     0.007127   \n",
       "False negative rate difference    0.003035           0.003272     0.007976   \n",
       "Equalized odds difference         0.020063           0.022357     0.007976   \n",
       "  ------                                                                     \n",
       "Overall AUC                       0.840792           0.692568     0.803496   \n",
       "AUC difference                    0.019052           0.012815     0.026507   \n",
       "\n",
       "                               GridSearch_13 GridSearch_20 GridSearch_21  \\\n",
       "Overall selection rate               0.72266      0.734849      0.735746   \n",
       "Demographic parity difference       0.000688      0.005063      0.009791   \n",
       "Demographic parity ratio            0.999048      0.993133      0.986777   \n",
       "------                                                                     \n",
       "Overall balanced error rate         0.763282      0.765257      0.766569   \n",
       "Balanced error rate difference      0.008621      0.012227      0.010204   \n",
       " ------                                                                    \n",
       "True positive rate                  0.847141      0.860262       0.86178   \n",
       "False positive rate difference      0.008828      0.018671      0.020287   \n",
       "False negative rate difference      0.008413      0.005782      0.000121   \n",
       "Equalized odds difference           0.008828      0.018671      0.020287   \n",
       "  ------                                                                   \n",
       "Overall AUC                         0.827145      0.836825       0.83926   \n",
       "AUC difference                      0.022811      0.020177      0.019253   \n",
       "\n",
       "                               GridSearch_29 GridSearch_31 GridSearch_46  \n",
       "Overall selection rate              0.735933      0.719894      0.697686  \n",
       "Demographic parity difference       0.003863      0.026939      0.006955  \n",
       "Demographic parity ratio            0.994764      0.963242      0.990079  \n",
       "------                                                                    \n",
       "Overall balanced error rate         0.764828      0.767684      0.745479  \n",
       "Balanced error rate difference      0.011107      0.009077      0.001762  \n",
       " ------                                                                   \n",
       "True positive rate                  0.861144      0.846455      0.813749  \n",
       "False positive rate difference       0.01577      0.035695      0.004964  \n",
       "False negative rate difference      0.006444      0.017541      0.001441  \n",
       "Equalized odds difference            0.01577      0.035695      0.004964  \n",
       "  ------                                                                  \n",
       "Overall AUC                         0.837586      0.837864      0.804387  \n",
       "AUC difference                      0.017592      0.016539      0.005434  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare GridSearch models with low values of equalized odds difference with the previously constructed models\n",
    "grid_search_dict = {\"GridSearch_{}\".format(i): (sweep_preds[i], sweep_scores[i])\n",
    "                    for i in range(len(sweep_preds))\n",
    "                    if non_dominated[i] and equalized_odds_sweep[i]<0.1}\n",
    "models_dict.update(grid_search_dict)\n",
    "get_metrics_df(models_dict, Y_test, B_str_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100d31b3",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we explored how a fairness-unaware gradient boosted trees model performed on the classification task in contrast to the postprocessed `ThresholdOptimizer` model and the `GridSearch` model. The `ThresholdOptimizer` greatly reduced the disparity in performance across multiple fairness metrics. However the overall error rate and AUC for the `ThresholdOptimizer` model were worse compared to the fairness-unaware model. \n",
    "\n",
    "With the `GridSearch` algorithm, we trained multiple models that balance the trade-off between the balanced accuracy and the equalized odds fairness metric. After engaging with relevant stakeholders, the data scientist can deploy the model that balances the performance-fairness trade-off that meets the needs of the business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d19805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
