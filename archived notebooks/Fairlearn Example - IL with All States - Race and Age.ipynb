{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cedc37c",
   "metadata": {},
   "source": [
    "# Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "639928ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Data processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Models\n",
    "# LightGBM is a gradient boosting framework that uses tree based learning algorithms\n",
    "import lightgbm as lgb\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Fairlearn algorithms and utils\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from fairlearn.reductions import GridSearch, EqualizedOdds\n",
    "\n",
    "# Metrics\n",
    "from fairlearn.metrics import (\n",
    "    MetricFrame,\n",
    "    selection_rate, demographic_parity_difference, demographic_parity_ratio,\n",
    "    true_positive_rate, false_positive_rate, false_negative_rate,\n",
    "    false_positive_rate_difference, false_negative_rate_difference,\n",
    "    equalized_odds_difference)\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26eda5d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>action_taken</th>\n",
       "      <th>preapproval_requested</th>\n",
       "      <th>loan_type</th>\n",
       "      <th>loan_purpose</th>\n",
       "      <th>interest_only_payment</th>\n",
       "      <th>balloon_payment</th>\n",
       "      <th>debt_to_income_ratio</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>loan_to_value_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ethnicity  race  gender  action_taken  preapproval_requested  loan_type  \\\n",
       "0          0     1       0             1                      0          1   \n",
       "1          0     1       1             1                      0          3   \n",
       "2          0     2       0             1                      0          1   \n",
       "3          0     1       0             1                      0          1   \n",
       "4          0     3       0             1                      0          1   \n",
       "\n",
       "   loan_purpose  interest_only_payment  balloon_payment  debt_to_income_ratio  \\\n",
       "0             1                      2                2                     3   \n",
       "1             3                      2                2                     4   \n",
       "2             3                      2                2                     4   \n",
       "3             3                      2                2                     2   \n",
       "4             1                      2                2                     4   \n",
       "\n",
       "   age  income  loan_to_value_ratio  \n",
       "0    3       1                    1  \n",
       "1    3       3                    1  \n",
       "2    2       3                    3  \n",
       "3    3       5                    1  \n",
       "4    3       2                    1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the clean data\n",
    "dc = 'data/Fairlearn_DC.csv'\n",
    "dc = pd.read_csv(dc, sep = ',')\n",
    "dc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c904690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>action_taken</th>\n",
       "      <th>preapproval_requested</th>\n",
       "      <th>loan_type</th>\n",
       "      <th>loan_purpose</th>\n",
       "      <th>interest_only_payment</th>\n",
       "      <th>balloon_payment</th>\n",
       "      <th>debt_to_income_ratio</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>loan_to_value_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ethnicity  race  gender  action_taken  preapproval_requested  loan_type  \\\n",
       "0          0     1       1             1                      0          1   \n",
       "1          0     1       1             1                      0          1   \n",
       "2          0     1       0             1                      0          1   \n",
       "3          0     1       0             1                      0          1   \n",
       "4          0     1       0             1                      0          1   \n",
       "\n",
       "   loan_purpose  interest_only_payment  balloon_payment  debt_to_income_ratio  \\\n",
       "0             1                      2                2                     3   \n",
       "1             1                      2                2                     4   \n",
       "2             1                      2                2                     4   \n",
       "3             3                      2                2                     2   \n",
       "4             1                      2                2                     2   \n",
       "\n",
       "   age  income  loan_to_value_ratio  \n",
       "0    3       1                    1  \n",
       "1    2       2                    3  \n",
       "2    2       3                    1  \n",
       "3    2       3                    1  \n",
       "4    3       2                    1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the clean data\n",
    "fl = 'data/Fairlearn_FL_PortSL.csv'\n",
    "fl = pd.read_csv(fl, sep = ',')\n",
    "fl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3a9abb6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>action_taken</th>\n",
       "      <th>preapproval_requested</th>\n",
       "      <th>loan_type</th>\n",
       "      <th>loan_purpose</th>\n",
       "      <th>interest_only_payment</th>\n",
       "      <th>balloon_payment</th>\n",
       "      <th>debt_to_income_ratio</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>loan_to_value_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ethnicity  race  gender  action_taken  preapproval_requested  loan_type  \\\n",
       "0          0     1       1             1                      0          1   \n",
       "1          0     1       1             0                      0          1   \n",
       "2          1     1       1             1                      0          1   \n",
       "3          0     1       1             1                      0          1   \n",
       "4          1     1       0             1                      0          1   \n",
       "\n",
       "   loan_purpose  interest_only_payment  balloon_payment  debt_to_income_ratio  \\\n",
       "0             1                      2                2                     4   \n",
       "1             3                      2                2                     4   \n",
       "2             1                      2                2                     4   \n",
       "3             1                      2                2                     1   \n",
       "4             1                      2                2                     4   \n",
       "\n",
       "   age  income  loan_to_value_ratio  \n",
       "0    1       2                    3  \n",
       "1    2       2                    1  \n",
       "2    2       3                    1  \n",
       "3    3       3                    3  \n",
       "4    2       4                    1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the clean data\n",
    "tx = 'data/Fairlearn_TX_Waco.csv'\n",
    "tx = pd.read_csv(tx, sep = ',')\n",
    "tx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a2e4fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2071 entries, 0 to 2070\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count  Dtype\n",
      "---  ------                 --------------  -----\n",
      " 0   ethnicity              2071 non-null   int64\n",
      " 1   race                   2071 non-null   int64\n",
      " 2   gender                 2071 non-null   int64\n",
      " 3   action_taken           2071 non-null   int64\n",
      " 4   preapproval_requested  2071 non-null   int64\n",
      " 5   loan_type              2071 non-null   int64\n",
      " 6   loan_purpose           2071 non-null   int64\n",
      " 7   interest_only_payment  2071 non-null   int64\n",
      " 8   balloon_payment        2071 non-null   int64\n",
      " 9   debt_to_income_ratio   2071 non-null   int64\n",
      " 10  age                    2071 non-null   int64\n",
      " 11  income                 2071 non-null   int64\n",
      " 12  loan_to_value_ratio    2071 non-null   int64\n",
      "dtypes: int64(13)\n",
      "memory usage: 210.5 KB\n"
     ]
    }
   ],
   "source": [
    "tx.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb74e5ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>action_taken</th>\n",
       "      <th>preapproval_requested</th>\n",
       "      <th>loan_type</th>\n",
       "      <th>loan_purpose</th>\n",
       "      <th>interest_only_payment</th>\n",
       "      <th>balloon_payment</th>\n",
       "      <th>debt_to_income_ratio</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>loan_to_value_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ethnicity  race  gender  action_taken  preapproval_requested  loan_type  \\\n",
       "0          0     1       0             1                      0          1   \n",
       "1          0     1       1             0                      0          1   \n",
       "2          0     1       1             1                      0          1   \n",
       "3          0     1       0             1                      0          1   \n",
       "4          1     1       1             1                      0          1   \n",
       "\n",
       "   loan_purpose  interest_only_payment  balloon_payment  debt_to_income_ratio  \\\n",
       "0             3                      2                2                     3   \n",
       "1             2                      2                2                     4   \n",
       "2             1                      2                2                     3   \n",
       "3             3                      2                2                     2   \n",
       "4             3                      2                2                     4   \n",
       "\n",
       "   age  income  loan_to_value_ratio  \n",
       "0    2       2                    1  \n",
       "1    3       4                    1  \n",
       "2    2       1                    1  \n",
       "3    2       2                    1  \n",
       "4    2       3                    1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the clean data\n",
    "il = 'data/Fairlearn_IL_Chicago.csv'\n",
    "il = pd.read_csv(il, sep = ',')\n",
    "il.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "522f8feb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 110882 entries, 0 to 110881\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count   Dtype\n",
      "---  ------                 --------------   -----\n",
      " 0   ethnicity              110882 non-null  int64\n",
      " 1   race                   110882 non-null  int64\n",
      " 2   gender                 110882 non-null  int64\n",
      " 3   action_taken           110882 non-null  int64\n",
      " 4   preapproval_requested  110882 non-null  int64\n",
      " 5   loan_type              110882 non-null  int64\n",
      " 6   loan_purpose           110882 non-null  int64\n",
      " 7   interest_only_payment  110882 non-null  int64\n",
      " 8   balloon_payment        110882 non-null  int64\n",
      " 9   debt_to_income_ratio   110882 non-null  int64\n",
      " 10  age                    110882 non-null  int64\n",
      " 11  income                 110882 non-null  int64\n",
      " 12  loan_to_value_ratio    110882 non-null  int64\n",
      "dtypes: int64(13)\n",
      "memory usage: 11.0 MB\n"
     ]
    }
   ],
   "source": [
    "il.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8747863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set = FL + DC + TX, testing set = IL\n",
    "all_df = pd.concat([dc, fl, tx, il])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77de1880",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 212714 entries, 0 to 110881\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count   Dtype\n",
      "---  ------                 --------------   -----\n",
      " 0   ethnicity              212714 non-null  int64\n",
      " 1   race                   212714 non-null  int64\n",
      " 2   gender                 212714 non-null  int64\n",
      " 3   action_taken           212714 non-null  int64\n",
      " 4   preapproval_requested  212714 non-null  int64\n",
      " 5   loan_type              212714 non-null  int64\n",
      " 6   loan_purpose           212714 non-null  int64\n",
      " 7   interest_only_payment  212714 non-null  int64\n",
      " 8   balloon_payment        212714 non-null  int64\n",
      " 9   debt_to_income_ratio   212714 non-null  int64\n",
      " 10  age                    212714 non-null  int64\n",
      " 11  income                 212714 non-null  int64\n",
      " 12  loan_to_value_ratio    212714 non-null  int64\n",
      "dtypes: int64(13)\n",
      "memory usage: 22.7 MB\n"
     ]
    }
   ],
   "source": [
    "all_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e19f9a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             White\n",
       "1             White\n",
       "2         Non-White\n",
       "3             White\n",
       "4         Non-White\n",
       "            ...    \n",
       "110877        White\n",
       "110878        White\n",
       "110879    Non-White\n",
       "110880        White\n",
       "110881        White\n",
       "Name: race, Length: 212714, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the sensitive feature - example: race\n",
    "A = all_df[\"race\"].apply(lambda x:1 if x == 1 else 0)\n",
    "A_str = A.map({ 1:\"White\", 0:\"Non-White\"})\n",
    "A_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d996aa14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 212714 entries, 0 to 110881\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count   Dtype   \n",
      "---  ------                 --------------   -----   \n",
      " 0   ethnicity              212714 non-null  int64   \n",
      " 1   race                   212714 non-null  int64   \n",
      " 2   gender                 212714 non-null  int64   \n",
      " 3   action_taken           212714 non-null  int64   \n",
      " 4   preapproval_requested  212714 non-null  category\n",
      " 5   loan_type              212714 non-null  category\n",
      " 6   loan_purpose           212714 non-null  category\n",
      " 7   interest_only_payment  212714 non-null  category\n",
      " 8   balloon_payment        212714 non-null  category\n",
      " 9   debt_to_income_ratio   212714 non-null  category\n",
      " 10  age                    212714 non-null  int64   \n",
      " 11  income                 212714 non-null  category\n",
      " 12  loan_to_value_ratio    212714 non-null  int64   \n",
      "dtypes: category(7), int64(6)\n",
      "memory usage: 12.8 MB\n"
     ]
    }
   ],
   "source": [
    "# Extract the target\n",
    "Y = all_df[\"action_taken\"]\n",
    "categorical_features = ['preapproval_requested', 'loan_type','loan_purpose', 'interest_only_payment', 'balloon_payment', 'debt_to_income_ratio', 'income']\n",
    "for col in categorical_features:\n",
    "    all_df[col] = all_df[col].astype('category')\n",
    "all_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "814b4685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101833"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "212714-110881"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5656aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "df_train = all_df.drop(columns=['race', 'action_taken', 'ethnicity', 'gender', 'age']).head(101833)\n",
    "df_test = all_df.drop(columns=['race', 'action_taken', 'ethnicity', 'gender', 'age']).tail(110881)\n",
    "Y_train = Y.head(101833)\n",
    "Y_test = Y.tail(110881)\n",
    "A_train = A.head(101833)\n",
    "A_test = A.tail(110881)\n",
    "A_str_train = A_str.head(101833)\n",
    "A_str_test = A_str.tail(110881)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d09fb70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 101833 entries, 0 to 0\n",
      "Data columns (total 8 columns):\n",
      " #   Column                 Non-Null Count   Dtype   \n",
      "---  ------                 --------------   -----   \n",
      " 0   preapproval_requested  101833 non-null  category\n",
      " 1   loan_type              101833 non-null  category\n",
      " 2   loan_purpose           101833 non-null  category\n",
      " 3   interest_only_payment  101833 non-null  category\n",
      " 4   balloon_payment        101833 non-null  category\n",
      " 5   debt_to_income_ratio   101833 non-null  category\n",
      " 6   income                 101833 non-null  category\n",
      " 7   loan_to_value_ratio    101833 non-null  int64   \n",
      "dtypes: category(7), int64(1)\n",
      "memory usage: 2.2 MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1888afba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 110881 entries, 1 to 110881\n",
      "Data columns (total 8 columns):\n",
      " #   Column                 Non-Null Count   Dtype   \n",
      "---  ------                 --------------   -----   \n",
      " 0   preapproval_requested  110881 non-null  category\n",
      " 1   loan_type              110881 non-null  category\n",
      " 2   loan_purpose           110881 non-null  category\n",
      " 3   interest_only_payment  110881 non-null  category\n",
      " 4   balloon_payment        110881 non-null  category\n",
      " 5   debt_to_income_ratio   110881 non-null  category\n",
      " 6   income                 110881 non-null  category\n",
      " 7   loan_to_value_ratio    110881 non-null  int64   \n",
      "dtypes: category(7), int64(1)\n",
      "memory usage: 2.4 MB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec8c5cb",
   "metadata": {},
   "source": [
    "## Using a Fairness Unaware Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69a19813",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'objective' : 'binary',\n",
    "    'metric' : 'auc',\n",
    "    'learning_rate': 0.03,\n",
    "    'num_leaves' : 10,\n",
    "    'max_depth' : 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ded3996",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(**lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74cffc68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(learning_rate=0.03, max_depth=3, metric='auc', num_leaves=10,\n",
       "               objective='binary')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(df_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cde8e4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores on test set\n",
    "test_scores = model.predict_proba(df_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37a4761a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8427116654982104"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train AUC\n",
    "roc_auc_score(Y_train, model.predict_proba(df_train)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acf9263c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions (0 or 1) on test set\n",
    "test_preds = (test_scores >= np.mean(Y_train)) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "08156ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LightGBM feature importance \n",
    "lgb.plot_importance(model, height=0.6, title=\"Features importance (LightGBM)\", importance_type=\"gain\", max_num_features=15) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ffffc3",
   "metadata": {},
   "source": [
    "We next use Fairlearn's `MetricFrame` to examine the the two different kinds of errors (false positives and false negatives) on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a520ac03",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\metrics\\_metric_frame.py:63: FutureWarning: You have provided 'metrics', 'y_true', 'y_pred' as positional arguments. Please pass them as keyword arguments. From version 0.10.0 passing them as positional arguments will result in an error.\n",
      "  warnings.warn(f\"You have provided {args_msg} as positional arguments. \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FPR</th>\n",
       "      <th>FNR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Non-White</th>\n",
       "      <td>0.346344</td>\n",
       "      <td>0.105009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White</th>\n",
       "      <td>0.337749</td>\n",
       "      <td>0.119245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                FPR       FNR\n",
       "race                         \n",
       "Non-White  0.346344  0.105009\n",
       "White      0.337749  0.119245"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mf = MetricFrame({\n",
    "    'FPR': false_positive_rate,\n",
    "    'FNR': false_negative_rate},\n",
    "    Y_test, test_preds, sensitive_features=A_str_test)\n",
    "\n",
    "mf.by_group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f9df958",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9ba60ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_series = pd.Series(test_preds)\n",
    "test_preds_series.reset_index(drop=True, inplace=True)\n",
    "A_str_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7fba1363",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_white = pd.concat([test_preds_series , A_str_test], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "85c53bc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110876</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110877</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110878</th>\n",
       "      <td>1</td>\n",
       "      <td>Non-White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110879</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110880</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110881 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       race\n",
       "0       0      White\n",
       "1       1      White\n",
       "2       1      White\n",
       "3       1      White\n",
       "4       1      White\n",
       "...    ..        ...\n",
       "110876  1      White\n",
       "110877  1      White\n",
       "110878  1  Non-White\n",
       "110879  1      White\n",
       "110880  1      White\n",
       "\n",
       "[110881 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69ea7bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_white.columns =['Unmitigated Result', 'Race']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ce01865f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unmitigated Result  Race     \n",
       "0                   Non-White     7675\n",
       "                    White        18895\n",
       "1                   Non-White    20037\n",
       "                    White        64274\n",
       "Name: Race, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_white.groupby(['Unmitigated Result','Race'])['Race'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf8fd828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def get_metrics_df(models_dict, y_true, group):\n",
    "    metrics_dict = {\n",
    "        \"Overall selection rate\": (\n",
    "            lambda x: selection_rate(y_true, x), True),\n",
    "        \"Demographic parity difference\": (\n",
    "            lambda x: demographic_parity_difference(y_true, x, sensitive_features=group), True),\n",
    "        \"Demographic parity ratio\": (\n",
    "            lambda x: demographic_parity_ratio(y_true, x, sensitive_features=group), True),\n",
    "        \"------\": (lambda x: \"\", True),\n",
    "        \"Overall balanced error rate\": (\n",
    "            lambda x: balanced_accuracy_score(y_true, x), True),\n",
    "        \"Balanced error rate difference\": (\n",
    "            lambda x: MetricFrame(metrics=balanced_accuracy_score, y_true=y_true, y_pred=x, sensitive_features=group).difference(method='between_groups'), True),\n",
    "        \" ------\": (lambda x: \"\", True),\n",
    "        # defined by team6 - equal opportunity - the percentage of people that have rightfully benefitted from the loan model\n",
    "        \"True positive rate\": (\n",
    "            lambda x: true_positive_rate(y_true, x), True),\n",
    "        # defined by team6 - equalized odds - the percentage of people that have wrongfully benefitted from the loan model\n",
    "        \"False positive rate difference\": (\n",
    "            lambda x: false_positive_rate_difference(y_true, x, sensitive_features=group), True),\n",
    "        \"False negative rate difference\": (\n",
    "            lambda x: false_negative_rate_difference(y_true, x, sensitive_features=group), True),\n",
    "        # defined by UCI Ex - 0 means that all groups have the same true positive, true negative, false positive, and false negative rates.)\n",
    "        \"Equalized odds difference\": (\n",
    "            lambda x: equalized_odds_difference(y_true, x, sensitive_features=group), True),\n",
    "        \"  ------\": (lambda x: \"\", True),\n",
    "        \"Overall AUC\": (\n",
    "            lambda x: roc_auc_score(y_true, x), False),\n",
    "        \"AUC difference\": (\n",
    "            lambda x: MetricFrame(metrics=roc_auc_score, y_true=y_true, y_pred=x, sensitive_features=group).difference(method='between_groups'), False)\n",
    "        \n",
    "    }\n",
    "    df_dict = {}\n",
    "    for metric_name, (metric_func, use_preds) in metrics_dict.items():\n",
    "        df_dict[metric_name] = [metric_func(preds) if use_preds else metric_func(scores) \n",
    "                                for model_name, (preds, scores) in models_dict.items()]\n",
    "    return pd.DataFrame.from_dict(df_dict, orient=\"index\", columns=models_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f0df44",
   "metadata": {},
   "source": [
    "We calculate several performance and fairness metrics below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d7642a4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unmitigated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Overall selection rate</th>\n",
       "      <td>0.760374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demographic parity difference</th>\n",
       "      <td>0.049768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demographic parity ratio</th>\n",
       "      <td>0.935602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall balanced error rate</th>\n",
       "      <td>0.771604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balanced error rate difference</th>\n",
       "      <td>0.00282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True positive rate</th>\n",
       "      <td>0.883917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False positive rate difference</th>\n",
       "      <td>0.008596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False negative rate difference</th>\n",
       "      <td>0.014236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equalized odds difference</th>\n",
       "      <td>0.014236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall AUC</th>\n",
       "      <td>0.8361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC difference</th>\n",
       "      <td>0.000358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Unmitigated\n",
       "Overall selection rate            0.760374\n",
       "Demographic parity difference     0.049768\n",
       "Demographic parity ratio          0.935602\n",
       "------                                    \n",
       "Overall balanced error rate       0.771604\n",
       "Balanced error rate difference     0.00282\n",
       " ------                                   \n",
       "True positive rate                0.883917\n",
       "False positive rate difference    0.008596\n",
       "False negative rate difference    0.014236\n",
       "Equalized odds difference         0.014236\n",
       "  ------                                  \n",
       "Overall AUC                         0.8361\n",
       "AUC difference                    0.000358"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metrics\n",
    "models_dict = {\"Unmitigated\": (test_preds, test_scores)}\n",
    "get_metrics_df(models_dict, Y_test, A_str_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd60941",
   "metadata": {},
   "source": [
    "## Mitigating Equalized Odds Difference with Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b266ba5b",
   "metadata": {},
   "source": [
    "We attempt to mitigate the disparities in the `lightgbm` predictions using the Fairlearn postprocessing algorithm `ThresholdOptimizer`. This algorithm finds a suitable threshold for the scores (class probabilities) produced by the `lightgbm` model by optimizing the accuracy rate under the constraint that the equalized odds difference (on training data) is zero. Since our goal is to optimize balanced accuracy, we resample the training data to have the same number of positive and negative examples. This means that `ThresholdOptimizer` is effectively optimizing balanced accuracy on the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bde592af",
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess_est = ThresholdOptimizer(\n",
    "    estimator=model,\n",
    "    constraints=\"equalized_odds\",\n",
    "    prefit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d5f05f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced data set is obtained by sampling the same number of points from the majority class (Y=0)\n",
    "# as there are points in the minority class (Y=1)\n",
    "balanced_idx1 = df_train[Y_train==1].index\n",
    "pp_train_idx = balanced_idx1.union(Y_train[Y_train==0].sample(n=balanced_idx1.size, random_state=1234, replace=True).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dbd3240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_balanced = df_train.loc[pp_train_idx, :]\n",
    "Y_train_balanced = Y_train.loc[pp_train_idx]\n",
    "A_train_balanced = A_train.loc[pp_train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7d52ebdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\postprocessing\\_threshold_optimizer.py:270: FutureWarning: 'predict_method' default value is changed from 'predict' to 'auto'. Explicitly pass `predict_method='predict' to replicate the old behavior, or pass `predict_method='auto' or other valid values to silence this warning.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ThresholdOptimizer(constraints='equalized_odds',\n",
       "                   estimator=LGBMClassifier(learning_rate=0.03, max_depth=3,\n",
       "                                            metric='auc', num_leaves=10,\n",
       "                                            objective='binary'),\n",
       "                   prefit=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postprocess_est.fit(df_train_balanced, Y_train_balanced, sensitive_features=A_train_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cc31b2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess_preds = postprocess_est.predict(df_test, sensitive_features=A_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2b5488df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unmitigated</th>\n",
       "      <th>ThresholdOptimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Overall selection rate</th>\n",
       "      <td>0.760374</td>\n",
       "      <td>0.891217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demographic parity difference</th>\n",
       "      <td>0.049768</td>\n",
       "      <td>0.034032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demographic parity ratio</th>\n",
       "      <td>0.935602</td>\n",
       "      <td>0.962175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall balanced error rate</th>\n",
       "      <td>0.771604</td>\n",
       "      <td>0.702147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balanced error rate difference</th>\n",
       "      <td>0.00282</td>\n",
       "      <td>0.010996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True positive rate</th>\n",
       "      <td>0.883917</td>\n",
       "      <td>0.983167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False positive rate difference</th>\n",
       "      <td>0.008596</td>\n",
       "      <td>0.028363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False negative rate difference</th>\n",
       "      <td>0.014236</td>\n",
       "      <td>0.006371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equalized odds difference</th>\n",
       "      <td>0.014236</td>\n",
       "      <td>0.028363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall AUC</th>\n",
       "      <td>0.8361</td>\n",
       "      <td>0.702147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC difference</th>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.010996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Unmitigated ThresholdOptimizer\n",
       "Overall selection rate            0.760374           0.891217\n",
       "Demographic parity difference     0.049768           0.034032\n",
       "Demographic parity ratio          0.935602           0.962175\n",
       "------                                                       \n",
       "Overall balanced error rate       0.771604           0.702147\n",
       "Balanced error rate difference     0.00282           0.010996\n",
       " ------                                                      \n",
       "True positive rate                0.883917           0.983167\n",
       "False positive rate difference    0.008596           0.028363\n",
       "False negative rate difference    0.014236           0.006371\n",
       "Equalized odds difference         0.014236           0.028363\n",
       "  ------                                                     \n",
       "Overall AUC                         0.8361           0.702147\n",
       "AUC difference                    0.000358           0.010996"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_dict = {\"Unmitigated\": (test_preds, test_scores),\n",
    "              \"ThresholdOptimizer\": (postprocess_preds, postprocess_preds)}\n",
    "get_metrics_df(models_dict, Y_test, A_str_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919f1698",
   "metadata": {},
   "source": [
    "The `ThresholdOptimizer` algorithm significantly reduces the disparity according to multiple metrics. However, the performance metrics (balanced error rate as well as AUC) get worse. Before deploying such a model in practice, it would be important to examine in more detail why we observe such a sharp trade-off. In our case it is because the available features are much less informative for one of the demographic groups than for the other.\n",
    "\n",
    "Note that unlike the unmitigated model, `ThresholdOptimizer` produces 0/1 predictions, so its balanced error rate difference is equal to the AUC difference, and its overall balanced error rate is equal to 1 - overall AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6712f85a",
   "metadata": {},
   "source": [
    "## Mitigating Equalized Odds Difference with GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22996117",
   "metadata": {},
   "source": [
    "We now attempt to mitigate disparities using the `GridSearch` algorithm. Unlike `ThresholdOptimizer`, the predictors produced by `GridSearch` do not access the sensitive feature at test time. Also, rather than training a single model, we train multiple models corresponding to different trade-off points between the performance metric (balanced accuracy) and fairness metric (equalized odds difference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "86022352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_moments\\utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:192: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.lambda_vecs_[i] = lambda_vec\n",
      "C:\\Users\\Xue Feng\\anaconda3\\lib\\site-packages\\fairlearn\\reductions\\_grid_search\\grid_search.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.gammas_[i] = self.constraints.gamma(predict_fct)\n"
     ]
    }
   ],
   "source": [
    "# Train GridSearch\n",
    "sweep = GridSearch(model,\n",
    "                   constraints=EqualizedOdds(),\n",
    "                   grid_size=1000,\n",
    "                   grid_limit=10)\n",
    "\n",
    "sweep.fit(df_train_balanced, Y_train_balanced, sensitive_features=A_train_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fdd2b76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_preds = [predictor.predict(df_test) for predictor in sweep.predictors_] \n",
    "sweep_scores = [predictor.predict_proba(df_test)[:, 1] for predictor in sweep.predictors_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e35a3b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized_odds_sweep = [\n",
    "    equalized_odds_difference(Y_test, preds, sensitive_features=A_str_test)\n",
    "    for preds in sweep_preds\n",
    "]\n",
    "balanced_accuracy_sweep = [balanced_accuracy_score(Y_test, preds) for preds in sweep_preds]\n",
    "auc_sweep = [roc_auc_score(Y_test, scores) for scores in sweep_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4914186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only non-dominated models (with respect to balanced accuracy and equalized odds difference)\n",
    "all_results = pd.DataFrame(\n",
    "    {\"predictor\": sweep.predictors_, \"accuracy\": balanced_accuracy_sweep, \"disparity\": equalized_odds_sweep}\n",
    ") \n",
    "non_dominated = [] \n",
    "for row in all_results.itertuples(): \n",
    "    accuracy_for_lower_or_eq_disparity = all_results[\"accuracy\"][all_results[\"disparity\"] <= row.disparity] \n",
    "    if row.accuracy >= accuracy_for_lower_or_eq_disparity.max(): \n",
    "        non_dominated.append(True)\n",
    "    else:\n",
    "        non_dominated.append(False)\n",
    "\n",
    "equalized_odds_sweep_non_dominated = np.asarray(equalized_odds_sweep)[non_dominated]\n",
    "balanced_accuracy_non_dominated = np.asarray(balanced_accuracy_sweep)[non_dominated]\n",
    "auc_non_dominated = np.asarray(auc_sweep)[non_dominated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "07127e31",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot equalized odds difference vs balanced accuracy\n",
    "plt.scatter(balanced_accuracy_non_dominated, equalized_odds_sweep_non_dominated, label=\"GridSearch Models\")\n",
    "plt.scatter(balanced_accuracy_score(Y_test, test_preds),\n",
    "            equalized_odds_difference(Y_test, test_preds, sensitive_features=A_str_test), \n",
    "            label=\"Unmitigated Model\")\n",
    "plt.scatter(balanced_accuracy_score(Y_test, postprocess_preds), \n",
    "            equalized_odds_difference(Y_test, postprocess_preds, sensitive_features=A_str_test),\n",
    "            label=\"ThresholdOptimizer Model\")\n",
    "plt.xlabel(\"Balanced Accuracy\")\n",
    "plt.ylabel(\"Equalized Odds Difference\")\n",
    "plt.legend(bbox_to_anchor=(1.55, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dff63d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21b386cb",
   "metadata": {},
   "source": [
    "As intended, `GridSearch` models appear along the trade-off curve between the large balanced accuracy (but also large disparity), and low disparity (but worse balanced accuracy). This gives the data scientist a flexibility to select a model that fits the application context best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6f895794",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot equalized odds difference vs AUC\n",
    "plt.scatter(auc_non_dominated, equalized_odds_sweep_non_dominated, label=\"GridSearch Models\")\n",
    "plt.scatter(roc_auc_score(Y_test, test_scores),\n",
    "            equalized_odds_difference(Y_test, test_preds, sensitive_features=A_str_test), \n",
    "            label=\"Unmitigated Model\")\n",
    "plt.scatter(roc_auc_score(Y_test, postprocess_preds), \n",
    "            equalized_odds_difference(Y_test, postprocess_preds, sensitive_features=A_str_test),\n",
    "            label=\"ThresholdOptimizer Model\")\n",
    "plt.xlabel(\"AUC\")\n",
    "plt.ylabel(\"Equalized Odds Difference\")\n",
    "plt.legend(bbox_to_anchor=(1.55, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8638e4",
   "metadata": {},
   "source": [
    "Similarly, `GridSearch` models appear along the trade-off curve between AUC and equalized odds difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cc88de81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unmitigated</th>\n",
       "      <th>ThresholdOptimizer</th>\n",
       "      <th>GridSearch_422</th>\n",
       "      <th>GridSearch_423</th>\n",
       "      <th>GridSearch_461</th>\n",
       "      <th>GridSearch_464</th>\n",
       "      <th>GridSearch_505</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Overall selection rate</th>\n",
       "      <td>0.760374</td>\n",
       "      <td>0.891217</td>\n",
       "      <td>0.781658</td>\n",
       "      <td>0.768671</td>\n",
       "      <td>0.788503</td>\n",
       "      <td>0.769041</td>\n",
       "      <td>0.786275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demographic parity difference</th>\n",
       "      <td>0.049768</td>\n",
       "      <td>0.034032</td>\n",
       "      <td>0.049807</td>\n",
       "      <td>0.047552</td>\n",
       "      <td>0.060521</td>\n",
       "      <td>0.046986</td>\n",
       "      <td>0.061015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demographic parity ratio</th>\n",
       "      <td>0.935602</td>\n",
       "      <td>0.962175</td>\n",
       "      <td>0.937279</td>\n",
       "      <td>0.93908</td>\n",
       "      <td>0.92469</td>\n",
       "      <td>0.939822</td>\n",
       "      <td>0.923876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall balanced error rate</th>\n",
       "      <td>0.771604</td>\n",
       "      <td>0.702147</td>\n",
       "      <td>0.768184</td>\n",
       "      <td>0.768813</td>\n",
       "      <td>0.76317</td>\n",
       "      <td>0.769591</td>\n",
       "      <td>0.765989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balanced error rate difference</th>\n",
       "      <td>0.00282</td>\n",
       "      <td>0.010996</td>\n",
       "      <td>0.00095</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.000258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True positive rate</th>\n",
       "      <td>0.883917</td>\n",
       "      <td>0.983167</td>\n",
       "      <td>0.903646</td>\n",
       "      <td>0.890945</td>\n",
       "      <td>0.90821</td>\n",
       "      <td>0.891669</td>\n",
       "      <td>0.907265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False positive rate difference</th>\n",
       "      <td>0.008596</td>\n",
       "      <td>0.028363</td>\n",
       "      <td>0.013206</td>\n",
       "      <td>0.014125</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.013814</td>\n",
       "      <td>0.000417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False negative rate difference</th>\n",
       "      <td>0.014236</td>\n",
       "      <td>0.006371</td>\n",
       "      <td>0.011306</td>\n",
       "      <td>0.014323</td>\n",
       "      <td>0.00017</td>\n",
       "      <td>0.015487</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equalized odds difference</th>\n",
       "      <td>0.014236</td>\n",
       "      <td>0.028363</td>\n",
       "      <td>0.013206</td>\n",
       "      <td>0.014323</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.015487</td>\n",
       "      <td>0.000417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>------</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall AUC</th>\n",
       "      <td>0.8361</td>\n",
       "      <td>0.702147</td>\n",
       "      <td>0.83392</td>\n",
       "      <td>0.830123</td>\n",
       "      <td>0.83589</td>\n",
       "      <td>0.830307</td>\n",
       "      <td>0.834469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC difference</th>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.010996</td>\n",
       "      <td>0.002484</td>\n",
       "      <td>0.00184</td>\n",
       "      <td>0.005087</td>\n",
       "      <td>0.001182</td>\n",
       "      <td>0.003529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Unmitigated ThresholdOptimizer GridSearch_422  \\\n",
       "Overall selection rate            0.760374           0.891217       0.781658   \n",
       "Demographic parity difference     0.049768           0.034032       0.049807   \n",
       "Demographic parity ratio          0.935602           0.962175       0.937279   \n",
       "------                                                                         \n",
       "Overall balanced error rate       0.771604           0.702147       0.768184   \n",
       "Balanced error rate difference     0.00282           0.010996        0.00095   \n",
       " ------                                                                        \n",
       "True positive rate                0.883917           0.983167       0.903646   \n",
       "False positive rate difference    0.008596           0.028363       0.013206   \n",
       "False negative rate difference    0.014236           0.006371       0.011306   \n",
       "Equalized odds difference         0.014236           0.028363       0.013206   \n",
       "  ------                                                                       \n",
       "Overall AUC                         0.8361           0.702147        0.83392   \n",
       "AUC difference                    0.000358           0.010996       0.002484   \n",
       "\n",
       "                               GridSearch_423 GridSearch_461 GridSearch_464  \\\n",
       "Overall selection rate               0.768671       0.788503       0.769041   \n",
       "Demographic parity difference        0.047552       0.060521       0.046986   \n",
       "Demographic parity ratio              0.93908        0.92469       0.939822   \n",
       "------                                                                        \n",
       "Overall balanced error rate          0.768813        0.76317       0.769591   \n",
       "Balanced error rate difference       0.000099        0.00005       0.000837   \n",
       " ------                                                                       \n",
       "True positive rate                   0.890945        0.90821       0.891669   \n",
       "False positive rate difference       0.014125       0.000271       0.013814   \n",
       "False negative rate difference       0.014323        0.00017       0.015487   \n",
       "Equalized odds difference            0.014323       0.000271       0.015487   \n",
       "  ------                                                                      \n",
       "Overall AUC                          0.830123        0.83589       0.830307   \n",
       "AUC difference                        0.00184       0.005087       0.001182   \n",
       "\n",
       "                               GridSearch_505  \n",
       "Overall selection rate               0.786275  \n",
       "Demographic parity difference        0.061015  \n",
       "Demographic parity ratio             0.923876  \n",
       "------                                         \n",
       "Overall balanced error rate          0.765989  \n",
       "Balanced error rate difference       0.000258  \n",
       " ------                                        \n",
       "True positive rate                   0.907265  \n",
       "False positive rate difference       0.000417  \n",
       "False negative rate difference         0.0001  \n",
       "Equalized odds difference            0.000417  \n",
       "  ------                                       \n",
       "Overall AUC                          0.834469  \n",
       "AUC difference                       0.003529  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare GridSearch models with low values of equalized odds difference with the previously constructed models\n",
    "grid_search_dict = {\"GridSearch_{}\".format(i): (sweep_preds[i], sweep_scores[i])\n",
    "                    for i in range(len(sweep_preds))\n",
    "                    if non_dominated[i] and equalized_odds_sweep[i]<0.1}\n",
    "models_dict.update(grid_search_dict)\n",
    "get_metrics_df(models_dict, Y_test, A_str_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d461b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chose the model with AUC = 83.01% from grid search\n",
    "test_pred_gird_search_423 = grid_search_dict.get(\"GridSearch_423\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fac1bb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_gird_search_423_series = pd.Series(test_pred_gird_search_423)\n",
    "test_pred_gird_search_423_series.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ceb80481",
   "metadata": {},
   "outputs": [],
   "source": [
    "migigated_pred_white = pd.concat([test_pred_gird_search_423_series , A_str_test], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b7334fd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110876</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110877</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110878</th>\n",
       "      <td>1</td>\n",
       "      <td>Non-White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110879</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110880</th>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110881 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       race\n",
       "0       0      White\n",
       "1       1      White\n",
       "2       1      White\n",
       "3       1      White\n",
       "4       1      White\n",
       "...    ..        ...\n",
       "110876  1      White\n",
       "110877  1      White\n",
       "110878  1  Non-White\n",
       "110879  1      White\n",
       "110880  1      White\n",
       "\n",
       "[110881 rows x 2 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "migigated_pred_white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "625cede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "migigated_pred_white.columns =['Mitigated Result', 'Race']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "98731cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mitigated Result  Race     \n",
       "0                 Non-White     7399\n",
       "                  White        18251\n",
       "1                 Non-White    20313\n",
       "                  White        64918\n",
       "Name: Race, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "migigated_pred_white.groupby(['Mitigated Result','Race'])['Race'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77965c56",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88127abe",
   "metadata": {},
   "source": [
    "In this notebook, we explored how a fairness-unaware gradient boosted trees model performed on the classification task in contrast to the postprocessed `ThresholdOptimizer` model and the `GridSearch` model. The `ThresholdOptimizer` greatly reduced the disparity in performance across multiple fairness metrics. However the overall error rate and AUC for the `ThresholdOptimizer` model were worse compared to the fairness-unaware model. \n",
    "\n",
    "With the `GridSearch` algorithm, we trained multiple models that balance the trade-off between the balanced accuracy and the equalized odds fairness metric. After engaging with relevant stakeholders, the data scientist can deploy the model that balances the performance-fairness trade-off that meets the needs of the business."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ef4cfe",
   "metadata": {},
   "source": [
    "# Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02b330e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
